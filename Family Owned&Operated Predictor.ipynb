{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18a8871c-6677-4d9d-a0ef-007eff7cf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, WebDriverException\n",
    ")\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "from typing import Dict, List, Tuple, Set, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fec732a8-7506-48e0-891a-40be86fd987b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Configuration\n",
    "# ---------------------------\n",
    "HEADLESS = True\n",
    "\n",
    "# Tighten timeouts to reduce long stalls (you can tune these)\n",
    "PAGE_LOAD_TIMEOUT = 14            # was 25\n",
    "IMPLICIT_WAIT = 0                 # use explicit waits instead\n",
    "WAIT_AFTER_NAV = 0.25             # was 2.0; rely on WebDriverWait\n",
    "\n",
    "# Hard per-site budget (seconds) to cap work; prevents pathological sites\n",
    "SITE_TIME_BUDGET = 180            \n",
    "RESTART_DRIVER_EVERY = 10        # restart webdriver after this many sites\n",
    "\n",
    "# --- Domain blacklist & SIDEARM detection ---\n",
    "DOMAIN_BLACKLIST = {\n",
    "    \"uwbadgers.com\",\n",
    "    \"sidearmsports.com\",\n",
    "    \"bigtenplus.com\",\n",
    "}\n",
    "\n",
    "def is_blacklisted(url: str) -> bool:\n",
    "    from urllib.parse import urlparse\n",
    "    host = urlparse(url).netloc.lower()\n",
    "    return any(host == d or host.endswith('.' + d) or host.endswith(d) for d in DOMAIN_BLACKLIST)\n",
    "\n",
    "def looks_like_sidearm(soup: BeautifulSoup) -> bool:\n",
    "    if not soup:\n",
    "        return False\n",
    "    html = str(soup).lower()\n",
    "    txt = (soup.get_text(separator=\" \").lower() if soup else \"\")\n",
    "    return any([\n",
    "        'data-sidearm-app' in html,\n",
    "        'pause all rotators' in txt,\n",
    "        'sidearm-icons.svg' in html,\n",
    "        'sidearmstats' in html,\n",
    "        'images.sidearmdev.com' in html,\n",
    "        'swiper ' in html\n",
    "    ])\n",
    "\n",
    "ABOUT_KEYWORDS = [\n",
    "    \"about\", \"about us\", \"our story\", \"who we are\", \"company\", \"mission\", \"vision\"\n",
    "]\n",
    "\n",
    "STRONG_LEADERSHIP_KEYWORDS = [\n",
    "    \"leadership\", \"executive\", \"executive-team\", \"management\", \"management-team\",\n",
    "    \"board\", \"board-of-directors\", \"principals\"\n",
    "]\n",
    "WEAK_LEADERSHIP_KEYWORDS = [\n",
    "    \"our team\", \"team\", \"people\", \"our people\", \"staff\"\n",
    "]\n",
    "LEADERSHIP_KEYWORDS = STRONG_LEADERSHIP_KEYWORDS + WEAK_LEADERSHIP_KEYWORDS\n",
    "\n",
    "COMMON_TITLES = {\n",
    "    \"ceo\", \"chief executive officer\", \"president\", \"chairman\", \"coo\", \"cfo\",\n",
    "    \"vice president\", \"vp\", \"senior vice president\", \"svp\", \"executive vice president\",\n",
    "    \"evp\", \"director\", \"managing director\", \"partner\", \"principal\", \"founder\",\n",
    "    \"co-founder\", \"chair\", \"board\", \"board chair\", \"board of directors\", \"advisor\",\n",
    "    \"general manager\", \"gm\", \"operations\", \"finance\", \"marketing\", \"strategy\",\n",
    "    \"people officer\", \"hr\", \"talent\", \"safety\", \"innovation\"\n",
    "}\n",
    "\n",
    "LEADERSHIP_TITLE_TOKENS = {\n",
    "    \"chief\", \"ceo\", \"coo\", \"cfo\", \"cto\", \"cio\", \"president\", \"chair\", \"chairman\",\n",
    "    \"vp\", \"vice president\", \"svp\", \"evp\", \"executive\", \"director\", \"principal\",\n",
    "    \"partner\", \"board\"\n",
    "}\n",
    "\n",
    "MENU_WORDS = {\n",
    "    \"news\", \"insights\", \"contact\", \"careers\", \"locations\",\n",
    "    \"services\", \"projects\", \"markets\", \"company\", \"home\", \"blog\", \"resources\"\n",
    "}\n",
    "\n",
    "NON_NAME_HEADINGS = {\n",
    "    \"our history\", \"history\", \"mission\", \"vision\", \"philosophy\", \"values\",\n",
    "    \"community\", \"locations\", \"services\", \"projects\", \"insights\"\n",
    "}\n",
    "\n",
    "BAD_CONTEXT_PATHS = {\n",
    "    \"/projects\", \"/project\", \"/insights\", \"/blog\", \"/resources\", \"/news\", \"/case-studies\", \"/events\"\n",
    "}\n",
    "\n",
    "PREFERRED_ABOUT_PATHS = {\n",
    "    \"/about\", \"/about-us\", \"/company\", \"/who-we-are\", \"/our-story\", \"/history\", \"/about-us/\"\n",
    "}\n",
    "\n",
    "PEOPLE_DIR_SLUGS = {\n",
    "    \"/people\", \"/our-people\", \"/eua-people\", \"/team\", \"/our-team\", \"/staff\"\n",
    "}\n",
    "\n",
    "NON_NAME_TERMS = {\n",
    "    \"services\",\"solutions\",\"tools\",\"machining\",\"casting\",\"alloy\",\"selector\",\n",
    "    \"comparison\",\"shop\",\"request\",\"quote\",\"careers\",\"news\",\"insights\",\"resources\",\n",
    "    \"projects\",\"contact\",\"privacy\",\"terms\"\n",
    "}\n",
    "\n",
    "# Limits to speed up heavy steps\n",
    "MAX_CAROUSEL_STEPS = 5\n",
    "MAX_PROFILES_TO_ENRICH = 8        # baseline; will adapt if needed\n",
    "\n",
    "# ---------------------------\n",
    "# News configuration\n",
    "# ---------------------------\n",
    "\n",
    "NEWS_KEYWORDS = [\n",
    "    \"news\", \"press\", \"press-releases\", \"media\",\n",
    "    \"company-news\", \"news-events\", \"announcements\", \"insights\"\n",
    "]\n",
    "\n",
    "# Cap how much we crawl/read\n",
    "MAX_NEWS_ARTICLES = 5           # first page only\n",
    "MAX_ARTICLE_BODY_CHARS = 2000   # per article body\n",
    "\n",
    "# ---------------------------\n",
    "# Imports\n",
    "# ---------------------------\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "import time, re, json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "\n",
    "def ensure_list(data):\n",
    "    return data if isinstance(data, list) else [data]\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = url.strip()\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        url = \"https://\" + url\n",
    "    return url\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def normalize_field(val) -> str:\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    if isinstance(val, list):\n",
    "        return clean_text(\" \".join(str(x) for x in val if x is not None))\n",
    "    if isinstance(val, dict):\n",
    "        for k in (\"name\", \"jobTitle\", \"url\", \"@id\"):\n",
    "            if k in val and val[k]:\n",
    "                return normalize_field(val[k])\n",
    "        return clean_text(str(val))\n",
    "    return clean_text(str(val))\n",
    "\n",
    "# ---------------------------\n",
    "# Driver helpers (Optimized)\n",
    "# ---------------------------\n",
    "\n",
    "def build_driver(headless: bool = True):\n",
    "    chrome_opts = Options()\n",
    "    if headless:\n",
    "        chrome_opts.add_argument(\"--headless=new\")\n",
    "\n",
    "    # SPEED: stop waiting for all subresources (DOMContentLoaded eager)\n",
    "    chrome_opts.page_load_strategy = \"eager\"\n",
    "\n",
    "    chrome_opts.add_argument(\"--disable-gpu\")\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--window-size=1400,1000\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_argument(\"--lang=en-US\")\n",
    "    chrome_opts.add_argument(\"--disable-extensions\")\n",
    "    chrome_opts.add_argument(\"--remote-allow-origins=*\")\n",
    "    chrome_opts.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_opts)\n",
    "    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "    driver.implicitly_wait(IMPLICIT_WAIT)\n",
    "\n",
    "    # Default: block heavy image formats using CDP (we’ll enable temporarily on leadership pages if needed)\n",
    "    try:\n",
    "        driver.execute_cdp_cmd(\"Network.enable\", {})\n",
    "        driver.execute_cdp_cmd(\"Network.setBlockedURLs\", {\"urls\": [\n",
    "            \"*.jpg\", \"*.jpeg\", \"*.png\", \"*.gif\", \"*.webp\", \"*.svg\",\n",
    "            \"*googletagmanager.com*\", \"*doubleclick.net*\", \"*gpt*\",\n",
    "            \"*sidearmstats*\", \"*images.sidearmdev.com*\", \"*cloudfront.net*/*sidearm*\",\n",
    "            \"*statbroadcast.com*\", \"*foxsports.com*\", \"*thevarsitynetwork.com*\"\n",
    "        ]})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return driver\n",
    "\n",
    "def set_image_loading(driver, enabled: bool):\n",
    "    \"\"\"Toggle image loading at runtime using CDP.\"\"\"\n",
    "    try:\n",
    "        driver.execute_cdp_cmd(\"Network.setBlockedURLs\", {\"urls\": [] if enabled else [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.gif\", \"*.webp\", \"*.svg\"]})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def safe_get(driver, url: str, timeout: int = None) -> bool:\n",
    "    \"\"\"Navigate with an enforced page-load timeout. Returns True on success, False on timeout/exception.\"\"\"\n",
    "    try:\n",
    "        if timeout is None:\n",
    "            timeout = PAGE_LOAD_TIMEOUT\n",
    "        driver.set_page_load_timeout(timeout)\n",
    "        driver.get(url)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        print(f\"[safe_get] Timeout loading {url}\")\n",
    "        return False\n",
    "    except WebDriverException as e:\n",
    "        print(f\"[safe_get] WebDriverException loading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def wait_for_body(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "def wait_for_content(driver, selectors: list[str], timeout: int = 8):\n",
    "    \"\"\"Explicit wait for any of the selectors; returns True/False.\"\"\"\n",
    "    end = time.time() + timeout\n",
    "    while time.time() < end:\n",
    "        try:\n",
    "            for sel in selectors:\n",
    "                if driver.find_elements(By.CSS_SELECTOR, sel):\n",
    "                    return True\n",
    "        except WebDriverException:\n",
    "            pass\n",
    "        time.sleep(0.2)\n",
    "    return False\n",
    "\n",
    "def prime_page_for_extraction(driver, cycles: int = 1, pause: float = 0.4):\n",
    "    \"\"\"SPEED: fewer scroll cycles to trigger lazy content only once.\"\"\"\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 0.6);\")\n",
    "        time.sleep(pause)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    except WebDriverException:\n",
    "        pass\n",
    "\n",
    "def deep_lazy_scroll(driver, cycles: int = 2, pause: float = 0.5):\n",
    "    \"\"\"SPEED: reduce cycles compared to original.\"\"\"\n",
    "    try:\n",
    "        for _ in range(cycles):\n",
    "            if site_time_remaining() <= 0:\n",
    "                break\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(pause)\n",
    "            if site_time_remaining() <= 0:\n",
    "                break\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(pause)\n",
    "    except WebDriverException:\n",
    "        pass\n",
    "\n",
    "def dismiss_cookie_banners(driver):\n",
    "    selectors = [\n",
    "        \"[id*='cookie'] button\", \"[class*='cookie'] button\",\n",
    "        \"[aria-label*='Accept']\", \"button[onclick*='accept']\",\n",
    "        \"button[title*='Accept']\"\n",
    "    ]\n",
    "    for sel in selectors[:3]:  # SPEED: check fewer patterns\n",
    "        try:\n",
    "            btns = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "            for b in btns[:2]:\n",
    "                driver.execute_script(\"arguments[0].click();\", b)\n",
    "                time.sleep(0.2)\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "def get_soup(driver) -> BeautifulSoup:\n",
    "    return BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "def get_main_container(soup: BeautifulSoup) -> BeautifulSoup:\n",
    "    candidates = soup.select(\"main, [role='main'], article, .page-content, #content, #main, .content\")\n",
    "    for c in candidates:\n",
    "        if c and len(clean_text(c.get_text())) > 50:\n",
    "            return c\n",
    "    return soup\n",
    "\n",
    "def is_in_excluded_area(el: BeautifulSoup) -> bool:\n",
    "    p = el\n",
    "    while p and hasattr(p, \"parent\"):\n",
    "        if p.name in {\"header\", \"nav\", \"footer\", \"aside\"}:\n",
    "            return True\n",
    "        classes = p.get(\"class\", []) or []\n",
    "        if any(cls in {\"site-header\", \"global-nav\", \"site-footer\", \"footer\", \"navbar\", \"menu\"} for cls in classes):\n",
    "            return True\n",
    "        p = p.parent\n",
    "    return False\n",
    "\n",
    "# ---------------------------\n",
    "# Leadership UI expansion (NEW)\n",
    "# ---------------------------\n",
    "\n",
    "def _query_contains(driver, selector: str, label: str):\n",
    "    \"\"\"\n",
    "    Emulate :contains('text') for simple text matches in Selenium.\n",
    "    selector = CSS selector w/o :contains, label = lowercase text to search.\n",
    "    \"\"\"\n",
    "    els = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "    out = []\n",
    "    for el in els:\n",
    "        try:\n",
    "            txt = (el.text or \"\").lower()\n",
    "            if label in txt:\n",
    "                out.append(el)\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "def expand_accordions(driver):\n",
    "    \"\"\"Click common accordion/collapse triggers to reveal bios/names.\"\"\"\n",
    "    triggers = driver.find_elements(By.CSS_SELECTOR,\n",
    "        \".accordion [aria-expanded='false'], .accordion button, .collapse-toggle, .accordion-header, .faq-item button\"\n",
    "    )\n",
    "    clicks = 0\n",
    "    for t in triggers:\n",
    "        if site_time_remaining() <= 0:\n",
    "            break\n",
    "        if clicks >= 6:\n",
    "            break\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", t)\n",
    "            time.sleep(0.2)\n",
    "            driver.execute_script(\"arguments[0].click();\", t)\n",
    "            clicks += 1\n",
    "            time.sleep(0.3)\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "def expand_leadership_ui(driver):\n",
    "    \"\"\"\n",
    "    Expand common UI controls on leadership/team pages to reveal all people:\n",
    "    - Tabs/filters (team, leadership, board)\n",
    "    - Load more / view all buttons\n",
    "    - Accordions\n",
    "    - Basic pagination (click “next” once or twice)\n",
    "    \"\"\"\n",
    "    # Tabs/filters\n",
    "    tab_selectors = [\n",
    "        \"[role='tab']\",\n",
    "        \".tabs [role='tab']\",\n",
    "        \".filter [role='tab']\",\n",
    "        \"[class*='tab'][class*='team']\",\n",
    "        \"[data-filter*='leadership']\",\n",
    "        \"[data-filter*='executive']\",\n",
    "        \"[data-filter*='board']\",\n",
    "        \"[data-filter*='team']\",\n",
    "        \".team-tabs button, .filters button\"\n",
    "    ]\n",
    "    # Load more / view all\n",
    "    load_selectors = [\n",
    "        \"button.load-more, a.load-more, .load-more button\",\n",
    "        \"button.view-all, a.view-all, .view-all button\",\n",
    "        \"button.show-more, a.show-more, .show-more button\",\n",
    "    ]\n",
    "    # Label-based controls (using contains-emulation)\n",
    "    label_controls = [\n",
    "        (\"button\", \"load more\"),\n",
    "        (\"a\", \"load more\"),\n",
    "        (\"button\", \"view all\"),\n",
    "        (\"a\", \"view all\"),\n",
    "        (\"button\", \"show more\"),\n",
    "        (\"a\", \"show more\"),\n",
    "    ]\n",
    "\n",
    "    # bail early if site budget exhausted\n",
    "    clicked = 0\n",
    "    if site_time_remaining() <= 0:\n",
    "        return\n",
    "    MAX_CLICKS = 8\n",
    "\n",
    "    # Try tab/filter selectors\n",
    "    for sel in tab_selectors:\n",
    "        if site_time_remaining() <= 0:\n",
    "            break\n",
    "        if clicked >= MAX_CLICKS:\n",
    "            break\n",
    "        try:\n",
    "            for el in driver.find_elements(By.CSS_SELECTOR, sel):\n",
    "                if site_time_remaining() <= 0:\n",
    "                    break\n",
    "                if clicked >= MAX_CLICKS:\n",
    "                    break\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                time.sleep(0.2)\n",
    "                driver.execute_script(\"arguments[0].click();\", el)\n",
    "                clicked += 1\n",
    "                time.sleep(0.4)\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "    # Try load-more/selectors\n",
    "    for sel in load_selectors:\n",
    "        if site_time_remaining() <= 0:\n",
    "            break\n",
    "        if clicked >= MAX_CLICKS:\n",
    "            break\n",
    "        try:\n",
    "            for el in driver.find_elements(By.CSS_SELECTOR, sel):\n",
    "                if site_time_remaining() <= 0:\n",
    "                    break\n",
    "                if clicked >= MAX_CLICKS:\n",
    "                    break\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                time.sleep(0.2)\n",
    "                driver.execute_script(\"arguments[0].click();\", el)\n",
    "                clicked += 1\n",
    "                time.sleep(0.5)\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "    # Try label-based contains controls\n",
    "    for base_sel, label in label_controls:\n",
    "        if site_time_remaining() <= 0:\n",
    "            break\n",
    "        if clicked >= MAX_CLICKS:\n",
    "            break\n",
    "        try:\n",
    "            candidates = _query_contains(driver, base_sel, label)\n",
    "            for el in candidates:\n",
    "                if site_time_remaining() <= 0:\n",
    "                    break\n",
    "                if clicked >= MAX_CLICKS:\n",
    "                    break\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                time.sleep(0.2)\n",
    "                driver.execute_script(\"arguments[0].click();\", el)\n",
    "                clicked += 1\n",
    "                time.sleep(0.5)\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "    # Expand accordions\n",
    "    expand_accordions(driver)\n",
    "\n",
    "    # Basic pagination (click “next” 1–2 times)\n",
    "    for _ in range(2):\n",
    "        if site_time_remaining() <= 0:\n",
    "            break\n",
    "        try:\n",
    "            next_candidates = driver.find_elements(By.CSS_SELECTOR, \".pagination a.next, a[rel='next'], a.next, .page-nav .next\")\n",
    "            if not next_candidates:\n",
    "                break\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", next_candidates[0])\n",
    "            time.sleep(0.2)\n",
    "            driver.execute_script(\"arguments[0].click();\", next_candidates[0])\n",
    "            wait_for_body(driver)\n",
    "            time.sleep(0.5)\n",
    "        except WebDriverException:\n",
    "            break\n",
    "\n",
    "# ---------------------------\n",
    "# Link discovery (minor speed tweaks)\n",
    "# ---------------------------\n",
    "\n",
    "def _path_segments(path: str) -> list[str]:\n",
    "    segs = [s for s in re.split(r\"[/_\\-]+\", path) if s]\n",
    "    return [s.lower() for s in segs]\n",
    "\n",
    "def _matches_segments(path: str, keywords: list[str]) -> bool:\n",
    "    segs = _path_segments(path)\n",
    "    kws = [k.lower().strip() for k in keywords]\n",
    "    return any(k in segs for k in kws)\n",
    "\n",
    "def _score_candidate_link(haystack_text: str, href_path: str, keywords: list[str], strong: bool) -> float:\n",
    "    score = 0.0\n",
    "    if any(k in haystack_text for k in keywords):\n",
    "        score += 1.0\n",
    "    if _matches_segments(href_path, keywords):\n",
    "        score += 2.5\n",
    "    for bad in BAD_CONTEXT_PATHS:\n",
    "        if bad in href_path:\n",
    "            score -= 1.5\n",
    "    if strong:\n",
    "        score += 0.3\n",
    "    return score\n",
    "\n",
    "def find_page_link_candidates(driver, keywords, strong: bool = False, exclude_paths=None) -> list[tuple[str,float]]:\n",
    "    exclude_paths = exclude_paths if exclude_paths is not None else BAD_CONTEXT_PATHS\n",
    "    anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    base = driver.current_url\n",
    "    base_domain = urlparse(base).netloc\n",
    "\n",
    "    candidates = []\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            href_abs = urljoin(base, href)\n",
    "            parsed = urlparse(href_abs)\n",
    "            href_path = (parsed.path or \"\").lower()\n",
    "            href_frag = (parsed.fragment or \"\")\n",
    "\n",
    "            if (not href_path) or href_path == \"/\" or href.startswith(\"#\") or href_frag:\n",
    "                continue\n",
    "\n",
    "            haystack = \" \".join([\n",
    "                (a.text or \"\"),\n",
    "                (a.get_attribute(\"aria-label\") or \"\"),\n",
    "                (a.get_attribute(\"title\") or \"\"),\n",
    "                (a.get_attribute(\"data-text\") or \"\")\n",
    "            ]).lower()\n",
    "\n",
    "            score = _score_candidate_link(haystack, href_path, keywords, strong)\n",
    "\n",
    "            if parsed.netloc != base_domain:\n",
    "                continue\n",
    "\n",
    "            if any(href_path.rstrip(\"/\") == slug for slug in PEOPLE_DIR_SLUGS):\n",
    "                score += 1.0\n",
    "\n",
    "            if _matches_segments(href_path.rstrip(\"/\"), [\"leadership\", \"executive\", \"team\", \"people\", \"board\", \"management\"]):\n",
    "                score += 0.5\n",
    "\n",
    "            if any(b in href_path for b in exclude_paths):\n",
    "                continue\n",
    "\n",
    "            candidates.append((href_abs, score))\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    uniq, seen = [], set()\n",
    "    for u, s in candidates[:8]:  # SPEED: cap candidates to examine\n",
    "        if u not in seen:\n",
    "            uniq.append((u,s)); seen.add(u)\n",
    "    return uniq\n",
    "\n",
    "# Site-level remaining time helper (uses SITE_DEADLINE set per-site in process_url)\n",
    "def site_time_remaining() -> float:\n",
    "    try:\n",
    "        return max(0.0, SITE_DEADLINE - time.time())\n",
    "    except Exception:\n",
    "        return SITE_TIME_BUDGET\n",
    "\n",
    "def choose_news_page(driver, base_url: str, debug: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    Pick a candidate news/press page from the landing page.\n",
    "    Strategy:\n",
    "      - score anchors by NEWS_KEYWORDS (internal domain only)\n",
    "      - skip bad contexts (blogs with pagination are OK, we will only take the first page)\n",
    "      - accept on first page visit if article cards are visible\n",
    "    \"\"\"\n",
    "    \n",
    "    cand = find_page_link_candidates(\n",
    "        driver,\n",
    "        NEWS_KEYWORDS,\n",
    "        strong=False,\n",
    "        exclude_paths=BAD_CONTEXT_PATHS - {\"/news\"}\n",
    "    )[:6]\n",
    "    debug[\"news_candidates\"] = [u for (u, _) in cand]\n",
    "    visited = set()\n",
    "\n",
    "    for u, _score in cand:\n",
    "        if u in visited:\n",
    "            continue\n",
    "        visited.add(u)\n",
    "        try:\n",
    "            ok = safe_get(driver, u)\n",
    "            if not ok:\n",
    "                debug.setdefault(\"news_errors\", []).append({\"url\": u, \"error\": \"timeout\"})\n",
    "                continue\n",
    "            wait_for_body(driver)\n",
    "            dismiss_cookie_banners(driver)\n",
    "\n",
    "            # look for typical WP archive markers\n",
    "            has_cards = wait_for_content(\n",
    "                driver,\n",
    "                selectors=[\n",
    "                    \"article\", \".wp-block-post\", \".post\", \".entry\", \".card\", \".grid-item\"\n",
    "                ],\n",
    "                timeout=6\n",
    "            )\n",
    "            if has_cards:\n",
    "                debug[\"news_accepted\"] = u\n",
    "                return u\n",
    "            else:\n",
    "                debug.setdefault(\"news_rejected\", []).append({\"url\": u, \"reason\": \"no_cards\"})\n",
    "        except Exception as e:\n",
    "            debug.setdefault(\"news_errors\", []).append({\"url\": u, \"error\": str(e)})\n",
    "            continue\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_about_link(driver) -> str | None:\n",
    "    anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    base = driver.current_url\n",
    "    base_domain = urlparse(base).netloc\n",
    "\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            href_abs = urljoin(base, href)\n",
    "            parsed = urlparse(href_abs)\n",
    "            path = (parsed.path or \"\").lower()\n",
    "            if parsed.netloc == base_domain and any(path.rstrip(\"/\") == p for p in PREFERRED_ABOUT_PATHS):\n",
    "                return href_abs\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "    cand = find_page_link_candidates(driver, ABOUT_KEYWORDS, strong=False)\n",
    "    for u, _ in cand:\n",
    "        path = urlparse(u).path.lower()\n",
    "        if any(b in path for b in BAD_CONTEXT_PATHS) and path not in PREFERRED_ABOUT_PATHS:\n",
    "            continue\n",
    "        return u\n",
    "    return None\n",
    "\n",
    "def click_keyword_button(driver, keywords: list[str]) -> bool:\n",
    "    base_url = driver.current_url\n",
    "    controls = driver.find_elements(By.CSS_SELECTOR, \"button, [role='button'], .btn, .button\")\n",
    "    for el in controls:\n",
    "        try:\n",
    "            text = \" \".join(filter(None, [\n",
    "                el.text or \"\",\n",
    "                el.get_attribute(\"aria-label\") or \"\",\n",
    "                el.get_attribute(\"title\") or \"\"\n",
    "            ])).lower()\n",
    "            attrs = \" \".join(filter(None, [\n",
    "                el.get_attribute(\"onclick\") or \"\",\n",
    "                el.get_attribute(\"data-url\") or \"\",\n",
    "                el.get_attribute(\"data-href\") or \"\"\n",
    "            ])).lower()\n",
    "\n",
    "            if any(k in text for k in keywords) or any(k in attrs for k in keywords):\n",
    "                driver.execute_script(\"arguments[0].click();\", el)\n",
    "                try:\n",
    "                    WebDriverWait(driver, 5).until(lambda d: d.current_url != base_url)\n",
    "                except TimeoutException:\n",
    "                    pass\n",
    "                return driver.current_url != base_url\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "# ---------------------------\n",
    "# Person signals & heuristics\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "PERSON_CARD_SELECTORS = [\n",
    "    # existing…\n",
    "    \".team-member\", \".leader\", \".staff\", \".person\", \".profile\",\n",
    "    \".member\", \".employee\", \".card\", \".tile\", \".grid-item\", \"article\",\n",
    "    \".profile-card\", \".bio-card\", \".team-card\", \".exec-card\", \".board-card\",\n",
    "    # NEW: common WordPress block containers that often hold name/title\n",
    "    \".wp-block-group\", \".wp-block-columns\", \".wp-block-column\", \".wp-block-post\"\n",
    "]\n",
    "\n",
    "PERSON_NAME_SELECTORS = [\n",
    "    \"[itemprop='name']\", \"[data-name]\", \".name\", \".person-name\", \".team-name\",\n",
    "    \".member-name\", \".profile-name\", \".card-title\", \".heading\", \"h2\", \"h3\", \"h4\", \"figcaption\", \".tile-start-name\"\n",
    "]\n",
    "PERSON_TITLE_SELECTORS = [\n",
    "    \"[itemprop='jobTitle']\", \"[data-title]\", \".title\", \".role\", \".position\",\n",
    "    \".job-title\", \".card-subtitle\", \".subtitle\", \".designation\", \"small\", \"em\", \"strong\", \".tile-start-title\"\n",
    "]\n",
    "\n",
    "def looks_like_title(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    t = text.strip().lower()\n",
    "    if any(tok in t for tok in COMMON_TITLES):\n",
    "        return True\n",
    "    return 3 < len(t) < 80 and not re.search(r\"[.!?]{2,}\", t)\n",
    "\n",
    "def looks_like_name(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    t = clean_text(text)\n",
    "    tl = t.lower()\n",
    "    for term in NON_NAME_TERMS:\n",
    "        if term in tl:\n",
    "            return False\n",
    "    if tl in NON_NAME_HEADINGS or tl.strip(\":\") in NON_NAME_HEADINGS:\n",
    "        return False\n",
    "    if tl in MENU_WORDS:\n",
    "        return False\n",
    "    if any(sym in t for sym in [\"/\", \"|\", \"+\"]):\n",
    "        return False\n",
    "    if len(t) > 80 or len(t) < 2:\n",
    "        return False\n",
    "\n",
    "    words = re.split(r\"\\s+\", t)\n",
    "    def is_proper(w):\n",
    "        return re.match(r\"^[A-Z][a-z'’\\-]+$|^[A-Z]\\.$|^O'[A-Z][a-z]+$|^[A-Z]{2,}$\", w) is not None\n",
    "    proper_tokens = sum(1 for w in words if is_proper(w))\n",
    "    return proper_tokens >= 2\n",
    "\n",
    "def is_leadership_title(title: str) -> bool:\n",
    "    t = (title or \"\").lower()\n",
    "    return any(tok in t for tok in LEADERSHIP_TITLE_TOKENS)\n",
    "\n",
    "# ---------- Role tokens / phrases & helpers (overlay-aware) ----------\n",
    "def norm_role(role_text: str) -> str:\n",
    "    r = role_text or \"\"\n",
    "    r = r.lower()\n",
    "    r = re.sub(r'[^a-z0-9\\s\\-+]', ' ', r)  # keep alnum, spaces, hyphen, plus\n",
    "    r = re.sub(r'\\s+', ' ', r).strip()\n",
    "    return r\n",
    "\n",
    "# Exact tokens (standalone words) -> leadership signal\n",
    "ROLE_TOKENS: Dict[str, float] = {\n",
    "    'owner': 2.2, 'co-owner': 2.0,\n",
    "    'founder': 1.5, 'co-founder': 1.4,\n",
    "    'president': 1.8,\n",
    "    'ceo': 1.6, 'coo': 1.2, 'cfo': 0.9,          # acronyms only as tokens\n",
    "    'principal': 1.0, 'partner': 1.0,\n",
    "    'chairman': 0.8, 'chair': 0.8,\n",
    "    'director': 0.5,                              # generic director modest\n",
    "}\n",
    "\n",
    "# Exact phrases (contiguous substrings) -> leadership signal\n",
    "ROLE_PHRASES: Dict[str, float] = {\n",
    "    'chief executive officer': 1.6,\n",
    "    'chief operating officer': 1.2,\n",
    "    'chief financial officer': 0.9,\n",
    "    'managing director': 1.5,\n",
    "    'general manager': 1.4,\n",
    "    'board member': 0.4,\n",
    "    'chairman of the board': 0.8,                 # CGSchmidt card phrase\n",
    "}\n",
    "\n",
    "# Blocklist: staff/support roles (never leadership)\n",
    "ROLE_BLOCKLIST = {'coordinator', 'assistant', 'specialist', 'administrator', 'ambassador', 'intern'}\n",
    "\n",
    "def role_weight_tokens(role_text: str) -> float:\n",
    "    \"\"\"Return leadership weight using tokens/phrases; 0 if blocklisted or no match.\"\"\"\n",
    "    r = norm_role(role_text)\n",
    "    tokens = set(r.split())\n",
    "    if any(b in tokens for b in ROLE_BLOCKLIST):\n",
    "        return 0.0\n",
    "    for tok, w in ROLE_TOKENS.items():\n",
    "        if tok in tokens:\n",
    "            return w\n",
    "    for phrase, w in ROLE_PHRASES.items():\n",
    "        if phrase in r:\n",
    "            return w\n",
    "    return 0.0\n",
    "\n",
    "def canonical_title(title: str) -> str:\n",
    "    \"\"\"Map variants to clean labels (e.g., 'President & CEO' remains as-is, phrases title-cased).\"\"\"\n",
    "    r = norm_role(title)\n",
    "    # phrase first\n",
    "    for phrase in ROLE_PHRASES:\n",
    "        if phrase in r:\n",
    "            return phrase.title()\n",
    "    # token fallback\n",
    "    tok_hits = [tok for tok in ROLE_TOKENS if tok in r.split()]\n",
    "    if tok_hits:\n",
    "        # Special-case combined roles\n",
    "        if 'president' in tok_hits and 'ceo' in tok_hits:\n",
    "            return 'President & CEO'\n",
    "        if 'coo' in tok_hits and 'executive' in r and 'vice' in r and 'president' in r:\n",
    "            return 'Chief Operating Officer, Executive Vice President'\n",
    "        # single token\n",
    "        return tok_hits[0].title()\n",
    "    return title.strip()\n",
    "\n",
    "# ---------------------------\n",
    "# JSON-LD & Microdata\n",
    "# ---------------------------\n",
    "\n",
    "def parse_jsonld_people(soup: BeautifulSoup, base_url: str) -> list[dict]:\n",
    "    leaders = []\n",
    "    for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(script.string or \"\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        items = data if isinstance(data, list) else [data]\n",
    "        for item in items:\n",
    "            graphs = item.get(\"@graph\", [])\n",
    "            candidates = graphs if isinstance(graphs, list) else []\n",
    "            if (normalize_field(item.get(\"@type\")) or \"\").lower() == \"person\":\n",
    "                candidates.append(item)\n",
    "\n",
    "            for p in candidates:\n",
    "                if (normalize_field(p.get(\"@type\")) or \"\").lower() != \"person\":\n",
    "                    continue\n",
    "                name  = normalize_field(p.get(\"name\", \"\"))\n",
    "                title = normalize_field(p.get(\"jobTitle\", \"\"))\n",
    "                bio   = normalize_field(p.get(\"description\", \"\"))\n",
    "                img   = normalize_field(p.get(\"image\", \"\"))\n",
    "                url   = normalize_field(p.get(\"url\", \"\"))\n",
    "                if name or title or bio:\n",
    "                    leaders.append({\n",
    "                        \"name\": name, \"title\": title, \"bio\": bio,\n",
    "                        \"image_url\": urljoin(base_url, img) if img else \"\",\n",
    "                        \"profile_url\": urljoin(base_url, url) if url else \"\",\n",
    "                        \"source\": \"jsonld\"\n",
    "                    })\n",
    "    return leaders\n",
    "\n",
    "# ---------------------------\n",
    "# Leadership extraction (Optimized order + limits)\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def find_leadership_sections(soup: BeautifulSoup) -> list[BeautifulSoup]:\n",
    "    sections = []\n",
    "    root = get_main_container(soup)\n",
    "\n",
    "    # existing heading-based section scan…\n",
    "    for section in root.find_all([\"section\", \"div\", \"article\"]):\n",
    "        if is_in_excluded_area(section):\n",
    "            continue\n",
    "        h = section.find([\"h1\", \"h2\", \"h3\", \"h4\"])\n",
    "        ht = clean_text(h.get_text()) if h else \"\"\n",
    "        if ht and any(k in ht.lower() for k in LEADERSHIP_KEYWORDS):\n",
    "            sections.append(section)\n",
    "\n",
    "    # existing id/class filters…\n",
    "    for sel in [\n",
    "        \"[id*='leader']\", \"[class*='leader']\",\n",
    "        \"[id*='team']\", \"[class*='team']\",\n",
    "        \"[id*='staff']\", \"[class*='staff']\",\n",
    "        \"[id*='people']\", \"[class*='people']\",\n",
    "        \"[id*='executive']\", \"[class*='executive']\",\n",
    "        \"[id*='management']\", \"[class*='management']\",\n",
    "        \"[id*='board']\", \"[class*='board']\",\n",
    "        # NEW: WP block containers near leadership sections\n",
    "        \".wp-block-group\", \".wp-block-columns\", \".wp-block-post\"\n",
    "    ]:\n",
    "        for section in root.select(sel):\n",
    "            if is_in_excluded_area(section):\n",
    "                continue\n",
    "            sections.append(section)\n",
    "\n",
    "    unique_sections, seen = [], set()\n",
    "    for s in sections:\n",
    "        key = id(s)\n",
    "        if key not in seen:\n",
    "            unique_sections.append(s); seen.add(key)\n",
    "    return unique_sections or [root]\n",
    "\n",
    "\n",
    "def extract_by_headings_nearby_title(soup: BeautifulSoup) -> list[dict]:\n",
    "    root = get_main_container(soup)\n",
    "    out = []\n",
    "\n",
    "    title_tokens = {\n",
    "        \"chief\", \"ceo\", \"coo\", \"cfo\", \"cto\", \"cio\",\n",
    "        \"president\", \"chair\", \"chairman\", \"vice president\",\n",
    "        \"vp\", \"svp\", \"evp\", \"executive vice president\",\n",
    "        \"senior vice president\", \"director\", \"principal\"\n",
    "    }\n",
    "\n",
    "    # Scan h2/h3/h4 that look like names\n",
    "    for h in root.find_all([\"h2\", \"h3\", \"h4\"]):\n",
    "        name = clean_text(h.get_text())\n",
    "        if not looks_like_name(name):\n",
    "            continue\n",
    "\n",
    "        # Walk a few following siblings to find a paragraph/div with title tokens\n",
    "        title = \"\"\n",
    "        sib = h\n",
    "        steps = 0\n",
    "        while sib and steps < 5:\n",
    "            sib = sib.find_next_sibling()\n",
    "            steps += 1\n",
    "            if not sib or sib.name not in {\"p\", \"div\", \"small\", \"em\", \"strong\"}:\n",
    "                continue\n",
    "            cand = clean_text(sib.get_text())\n",
    "            # Remove quoted pull‑quotes; keep first sentence/line\n",
    "            cand = cand.split(\"\\n\")[0].split(\".”\")[0].split(\".”\")[0]\n",
    "            cl = cand.lower()\n",
    "            if any(tok in cl for tok in title_tokens) and looks_like_title(cand):\n",
    "                title = cand\n",
    "                break\n",
    "\n",
    "        if not title:\n",
    "            # Look downward inside the same parent container for a p/div containing a title token\n",
    "            parent = h.find_parent([\"div\", \"section\", \"article\"]) or root\n",
    "            for el in parent.find_all([\"p\", \"div\", \"small\", \"em\", \"strong\"], limit=6):\n",
    "                cand = clean_text(el.get_text())\n",
    "                cl = cand.lower()\n",
    "                if any(tok in cl for tok in title_tokens) and looks_like_title(cand):\n",
    "                    title = cand\n",
    "                    break\n",
    "\n",
    "        if title:\n",
    "            out.append({\"name\": name, \"title\": title, \"bio\": \"\", \"image_url\": \"\", \"profile_url\": \"\"})\n",
    "\n",
    "    # De‑dup by (name,title)\n",
    "    uniq = {}\n",
    "    for L in out:\n",
    "        key = (normalize_field(L[\"name\"]).lower(), normalize_field(L[\"title\"]).lower())\n",
    "        if key not in uniq:\n",
    "            uniq[key] = L\n",
    "    return list(uniq.values())\n",
    "\n",
    "def extract_cards_from_container(container: BeautifulSoup, base_url: str) -> list[dict]:\n",
    "    leaders = []\n",
    "    for sel in PERSON_CARD_SELECTORS:\n",
    "        for card in container.select(sel):\n",
    "            raw_text = clean_text(card.get_text())\n",
    "            if len(raw_text) < 5:\n",
    "                continue\n",
    "\n",
    "            name_el = None\n",
    "            for s in PERSON_NAME_SELECTORS:\n",
    "                cand = card.select_one(s)\n",
    "                text = clean_text(cand.get_text() if cand else \"\")\n",
    "                if text and looks_like_name(text):\n",
    "                    name_el = cand\n",
    "                    break\n",
    "\n",
    "            title_el = None\n",
    "            for s in PERSON_TITLE_SELECTORS:\n",
    "                cand = card.select_one(s)\n",
    "                text = clean_text(cand.get_text() if cand else \"\")\n",
    "                if text and looks_like_title(text):\n",
    "                    title_el = cand\n",
    "                    break\n",
    "            if not title_el:\n",
    "                continue\n",
    "\n",
    "            bio_el = None\n",
    "            for s in [\".bio\", \".summary\", \".description\", \"[itemprop='description']\"]:\n",
    "                cand = card.select_one(s)\n",
    "                if cand and len(clean_text(cand.get_text())) > 30:\n",
    "                    bio_el = cand\n",
    "                    break\n",
    "\n",
    "            img_el = card.select_one(\"img\")\n",
    "            img_src = \"\"\n",
    "            if img_el:\n",
    "                for attr in [\"src\", \"data-src\", \"data-original\", \"data-image\"]:\n",
    "                    val = img_el.get(attr)\n",
    "                    if val:\n",
    "                        img_src = val\n",
    "                        break\n",
    "\n",
    "            a_el = card.select_one(\"a[href]\")\n",
    "            href  = a_el.get(\"href\") if a_el else \"\"\n",
    "\n",
    "            name  = clean_text(name_el.get_text()) if name_el else \"\"\n",
    "            title = clean_text(title_el.get_text())\n",
    "            bio   = clean_text(bio_el.get_text()) if bio_el else \"\"\n",
    "\n",
    "            if not name and img_el:\n",
    "                img_alt = clean_text(img_el.get(\"alt\",\"\"))\n",
    "                if img_alt and looks_like_name(img_alt):\n",
    "                    name = img_alt\n",
    "\n",
    "            if not name:\n",
    "                continue\n",
    "\n",
    "            leaders.append({\n",
    "                \"name\": name,\n",
    "                \"title\": title,\n",
    "                \"bio\": bio[:2000] if bio else \"\",\n",
    "                \"image_url\": urljoin(base_url, img_src) if img_src else \"\",\n",
    "                \"profile_url\": urljoin(base_url, href) if href else \"\",\n",
    "            })\n",
    "    return leaders\n",
    "\n",
    "def harvest_name_title_from_headings(container: BeautifulSoup) -> list[dict]:\n",
    "    leaders = []\n",
    "    for h in container.find_all([\"h2\", \"h3\", \"h4\", \"figcaption\"]):\n",
    "        name = clean_text(h.get_text())\n",
    "        if not looks_like_name(name):\n",
    "            continue\n",
    "        title = \"\"\n",
    "        sib = h.find_next_sibling()\n",
    "        while sib and sib.name in [\"br\", \"hr\"]:\n",
    "            sib = sib.find_next_sibling()\n",
    "        if sib:\n",
    "            candidate = clean_text(sib.get_text())\n",
    "            if looks_like_title(candidate):\n",
    "                title = candidate\n",
    "        if not title:\n",
    "            for sel in [\".title\", \".role\", \".position\", \".job-title\", \"small\", \"em\", \"strong\"]:\n",
    "                el = h.find_next(sel)\n",
    "                if el:\n",
    "                    candidate = clean_text(el.get_text())\n",
    "                    if looks_like_title(candidate):\n",
    "                        title = candidate\n",
    "                        break\n",
    "        if not title:\n",
    "            continue\n",
    "        leaders.append({\"name\": name, \"title\": title, \"bio\": \"\", \"image_url\": \"\", \"profile_url\": \"\"})\n",
    "    return leaders\n",
    "\n",
    "\n",
    "def harvest_from_images(container: BeautifulSoup, base_url: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Overlay-first card parser for 'tile' components:\n",
    "    - Read .tile-start-name and .tile-start-title directly from the grid.\n",
    "    - Ignore image alt (e.g., 'RickSchmidt-Bio-Mobile' is not a name).\n",
    "    - Do not visit profiles for titles; only attach href for later bio enrichment if needed.\n",
    "    \"\"\"\n",
    "    leaders = []\n",
    "    # Limit scope to tiles (owl carousel/card grid)\n",
    "    for tile in container.select(\".tile\"):\n",
    "        # Explicit overlay selectors (CGSchmidt structure)\n",
    "        name_el  = tile.select_one(\".tile-start-name\")\n",
    "        title_el = tile.select_one(\".tile-start-title\")\n",
    "\n",
    "        name  = clean_text(name_el.get_text()) if name_el else \"\"\n",
    "        title = clean_text(title_el.get_text()) if title_el else \"\"\n",
    "\n",
    "        # Fallback: parse visible lines when classes differ\n",
    "        if not (name and title):\n",
    "            raw = clean_text(tile.get_text(separator=\"\\n\"))\n",
    "            if not name:\n",
    "                # first name-like line\n",
    "                for line in [l for l in re.split(r\"\\s*\\n+\\s*\", raw) if l.strip()]:\n",
    "                    if looks_like_name(line):\n",
    "                        name = line.strip()\n",
    "                        break\n",
    "            if not title:\n",
    "                # scan next few lines after name for a leadership title (COO != Coordinator)\n",
    "                lines = [l for l in re.split(r\"\\s*\\n+\\s*\", raw) if l.strip()]\n",
    "                if name:\n",
    "                    try:\n",
    "                        i = lines.index(name)\n",
    "                        for j in range(i+1, min(len(lines), i+5)):\n",
    "                            if role_weight_tokens(lines[j]) > 0.0:\n",
    "                                title = lines[j].strip()\n",
    "                                break\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "        # Validate and canonicalize\n",
    "        if not looks_like_name(name):\n",
    "            continue\n",
    "        if role_weight_tokens(title) <= 0.0:\n",
    "            continue\n",
    "        title = canonical_title(title)\n",
    "\n",
    "        # Optional image/profile link (do not rely on profile for title)\n",
    "        img_el = tile.select_one(\"img\")\n",
    "        img_src = \"\"\n",
    "        if img_el:\n",
    "            for attr in [\"src\", \"data-src\", \"data-original\", \"data-image\"]:\n",
    "                val = img_el.get(attr)\n",
    "                if val:\n",
    "                    img_src = val\n",
    "                    break\n",
    "\n",
    "        a_el = tile.select_one(\"a.cta[href], a[href]\")  # 'Learn More' link\n",
    "        href  = a_el.get(\"href\") if a_el else \"\"\n",
    "\n",
    "        leaders.append({\n",
    "            \"name\": name,\n",
    "            \"title\": title,\n",
    "            \"bio\": \"\",  # bio can be enriched later, but we won't overwrite title from profiles\n",
    "            \"image_url\": urljoin(base_url, img_src) if img_src else \"\",\n",
    "            \"profile_url\": urljoin(base_url, href) if href else \"\",\n",
    "        })\n",
    "    return leaders\n",
    "\n",
    "\n",
    "def extract_leaders_from_text_blocks(soup: BeautifulSoup, base_url: str) -> list[dict]:\n",
    "    leaders = []\n",
    "    root = get_main_container(soup)\n",
    "    def find_name_title_pairs_in_text(block_text: str) -> list[tuple[str, str]]:\n",
    "        out = []\n",
    "        if not block_text:\n",
    "            return out\n",
    "        parts = re.split(r\"[;\\n\\r]+| — | - \", block_text)\n",
    "        for part in parts:\n",
    "            part = clean_text(part)\n",
    "            if not part:\n",
    "                continue\n",
    "            if re.search(r\"[,–—-]\", part):\n",
    "                tokens = re.split(r\"[,–—-]\", part, maxsplit=1)\n",
    "                name_candidate = clean_text(tokens[0]) if tokens else \"\"\n",
    "                title_candidate = clean_text(tokens[1]) if len(tokens) > 1 else \"\"\n",
    "            else:\n",
    "                tokens = part.split()\n",
    "                name_candidate = clean_text(\" \".join(tokens[:4]))\n",
    "                title_candidate = clean_text(\" \".join(tokens[4:]))\n",
    "            if looks_like_name(name_candidate) and looks_like_title(title_candidate):\n",
    "                out.append((name_candidate, title_candidate))\n",
    "        return out\n",
    "\n",
    "    for el in root.find_all([\"h2\", \"h3\", \"h4\", \"p\", \"li\"]):\n",
    "        # Header-followed-by-header pattern: e.g. <h3>NAME</h3>\\n<h4>TITLE</h4>\n",
    "        try:\n",
    "            if el.name in (\"h2\", \"h3\", \"h4\"):\n",
    "                nxt = el.find_next_sibling()\n",
    "                # skip over non-tags/text nodes\n",
    "                while nxt is not None and getattr(nxt, 'name', None) is None:\n",
    "                    nxt = nxt.find_next_sibling()\n",
    "                if nxt is not None and getattr(nxt, 'name', None) in (\"h2\", \"h3\", \"h4\", \"p\", \"div\", \"span\"):\n",
    "                    name_cand = clean_text(el.get_text())\n",
    "                    title_cand = clean_text(nxt.get_text())\n",
    "                    if looks_like_name(name_cand) and looks_like_title(title_cand):\n",
    "                        leaders.append({\"name\": name_cand, \"title\": title_cand, \"bio\": \"\", \"image_url\": \"\", \"profile_url\": \"\"})\n",
    "                        # continue to next element to avoid double-count\n",
    "                        continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        text = clean_text(el.get_text())\n",
    "        pairs = find_name_title_pairs_in_text(text)\n",
    "        for name, title in pairs:\n",
    "            leaders.append({\n",
    "                \"name\": name, \"title\": title, \"bio\": \"\",\n",
    "                \"image_url\": \"\", \"profile_url\": \"\"\n",
    "            })\n",
    "\n",
    "    uniq = {}\n",
    "    for L in leaders:\n",
    "        key = (normalize_field(L[\"name\"]).lower(), normalize_field(L[\"title\"]).lower())\n",
    "        if key not in uniq:\n",
    "            uniq[key] = L\n",
    "    return list(uniq.values())\n",
    "\n",
    "def collect_profile_links(container: BeautifulSoup, base_url: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Broadened to catch WP/team patterns and profile pages more reliably.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    BAD_PATHS = (\"/contact\", \"/careers\", \"/news\", \"/locations\", \"/services\", \"/markets\", \"/projects\")\n",
    "    BAD_EXACT = {\"#\", \"/\", \"\"}\n",
    "\n",
    "    for a in container.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        href_abs = urljoin(base_url, href)\n",
    "        low = href_abs.lower()\n",
    "        txt = clean_text(a.get_text())\n",
    "\n",
    "        # Strong signals\n",
    "        if any(k in low for k in [\"leadership\", \"executive\", \"team\", \"people\", \"staff\", \"board\", \"profile\", \"member\", \"bio\"]):\n",
    "            links.append(href_abs); continue\n",
    "        if looks_like_name(txt):\n",
    "            links.append(href_abs); continue\n",
    "\n",
    "        parsed = urlparse(href_abs)\n",
    "        path_low = (parsed.path or \"\").lower()\n",
    "        if href in BAD_EXACT:\n",
    "            continue\n",
    "        if any(path_low.rstrip(\"/\").endswith(bp) or path_low == bp for bp in BAD_PATHS):\n",
    "            continue\n",
    "\n",
    "        # WP-ish and team directories\n",
    "        if any(s in path_low for s in [\"/company/\", \"/about/\", \"/team/\", \"/people/\", \"/leadership/\", \"/board/\", \"/staff/\", \"/bio/\", \"/profiles/\", \"/members/\"]):\n",
    "            links.append(href_abs)\n",
    "\n",
    "    # Also harvest links from known containers (fallback)\n",
    "    for sel in [\".wp-block-post a\", \".wp-block-group a\", \".team-list a\", \".person-card a\", \".grid a\"]:\n",
    "        for a in container.select(sel):\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                links.append(urljoin(base_url, href))\n",
    "\n",
    "    # uniquify\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            uniq.append(u); seen.add(u)\n",
    "    return uniq\n",
    "\n",
    "def enrich_from_profiles(driver, profile_links: list[str], base_url: str, max_profiles: int = MAX_PROFILES_TO_ENRICH) -> list[dict]:\n",
    "    results = []\n",
    "    for link in profile_links[:max_profiles]:\n",
    "        if site_time_remaining() <= 0:\n",
    "            break\n",
    "        try:\n",
    "            path_low = urlparse(link).path.lower()\n",
    "            if any(b in path_low for b in BAD_CONTEXT_PATHS):\n",
    "                continue\n",
    "\n",
    "            ok = safe_get(driver, link)\n",
    "            if not ok:\n",
    "                continue\n",
    "            wait_for_body(driver)\n",
    "            dismiss_cookie_banners(driver)\n",
    "\n",
    "            soup = get_soup(driver)\n",
    "            name = \"\"\n",
    "            for tag in [\"h1\", \"h2\", \"h3\"]:\n",
    "                el = soup.find(tag)\n",
    "                if el:\n",
    "                    cand = clean_text(el.get_text())\n",
    "                    if looks_like_name(cand):\n",
    "                        name = cand; break\n",
    "            if not name:\n",
    "                img = soup.find(\"img\")\n",
    "                img_alt = clean_text(img.get(\"alt\",\"\")) if img else \"\"\n",
    "                if looks_like_name(img_alt):\n",
    "                    name = img_alt\n",
    "\n",
    "            title = \"\"\n",
    "            for sel in [\".title\", \".role\", \".position\", \".job-title\", \"strong\", \"em\"]:\n",
    "                el = soup.select_one(sel)\n",
    "                if el:\n",
    "                    cand = clean_text(el.get_text())\n",
    "                    if looks_like_title(cand):\n",
    "                        title = cand; break\n",
    "            if not title:\n",
    "                p = soup.find(\"p\")\n",
    "                cand = clean_text(p.get_text()) if p else \"\"\n",
    "                cand = cand.split(\".\")[0][:120]\n",
    "                if looks_like_title(cand):\n",
    "                    title = cand\n",
    "\n",
    "            if not (name and title):\n",
    "                continue\n",
    "\n",
    "            bio = \"\"\n",
    "            for sel in [\".bio\", \".summary\", \".description\"]:\n",
    "                el = soup.select_one(sel)\n",
    "                if el and len(clean_text(el.get_text())) > 30:\n",
    "                    bio = clean_text(el.get_text()); break\n",
    "\n",
    "            results.append({\n",
    "                \"name\": name, \"title\": title, \"bio\": bio,\n",
    "                \"image_url\": \"\", \"profile_url\": link\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def filter_to_likely_leaders(leaders: list[dict], page_url: str) -> list[dict]:\n",
    "    path = urlparse(page_url).path.lower()\n",
    "    if any(seg in path for seg in [\"/people\", \"/team\", \"/our-team\", \"/staff\", \"/our-people\"]):\n",
    "        return [L for L in leaders if is_leadership_title(normalize_field(L.get(\"title\",\"\")))]\n",
    "    return leaders\n",
    "\n",
    "def dedup_leaders(leaders: list[dict]) -> list[dict]:\n",
    "    uniq = {}\n",
    "    for L in leaders:\n",
    "        L[\"name\"] = normalize_field(L.get(\"name\",\"\"))\n",
    "        L[\"title\"] = normalize_field(L.get(\"title\",\"\"))\n",
    "        L[\"profile_url\"] = normalize_field(L.get(\"profile_url\",\"\"))\n",
    "        key = (L[\"name\"].lower(), L[\"profile_url\"].lower())\n",
    "        if not key[0]:\n",
    "            continue\n",
    "        if key not in uniq:\n",
    "            uniq[key] = L\n",
    "        else:\n",
    "            if not uniq[key].get(\"title\") and L.get(\"title\"):\n",
    "                uniq[key] = L\n",
    "    return list(uniq.values())\n",
    "\n",
    "def validate_leaders(leaders: list[dict]) -> bool:\n",
    "    valid = []\n",
    "    for L in leaders:\n",
    "        name = normalize_field(L.get(\"name\",\"\"))\n",
    "        title = normalize_field(L.get(\"title\",\"\"))\n",
    "        source = L.get(\"source\",\"\")\n",
    "        if (looks_like_name(name) and looks_like_title(title)) or (source == \"jsonld\" and (name or title)):\n",
    "            valid.append(L)\n",
    "    return len(valid) >= 2\n",
    "\n",
    "# ---------------------------\n",
    "# Carousel extraction (Limited)\n",
    "# ---------------------------\n",
    "\n",
    "def extract_from_carousel(driver, base_url: str) -> list[dict]:\n",
    "    leaders = []\n",
    "\n",
    "    carousel_wrappers = [\n",
    "        \".slick-slider\", \".slick-carousel\", \".slick-list\",\n",
    "        \".swiper\", \".swiper-container\", \".swiper-wrapper\",\n",
    "        \".owl-carousel\", \".owl-stage\", \".owl-stage-outer\",\n",
    "        \".carousel\", \".carousel-inner\", \".carousel-container\"\n",
    "    ]\n",
    "    slide_selectors = [\".slick-slide\", \".swiper-slide\", \".owl-item\", \".carousel-item\", \".slide\"]\n",
    "    next_btn_selectors = [\".slick-next\", \".swiper-button-next\", \".owl-next\", \".carousel-control-next\", \".next\", \"[aria-label='Next']\"]\n",
    "\n",
    "    def parse_slide_html(html: str) -> list[dict]:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        return extract_cards_from_container(soup, base_url)\n",
    "\n",
    "    carousels = []\n",
    "    for sel in carousel_wrappers:\n",
    "        try:\n",
    "            carousels.extend(driver.find_elements(By.CSS_SELECTOR, sel))\n",
    "        except WebDriverException:\n",
    "            continue\n",
    "\n",
    "    if not carousels:\n",
    "        return []\n",
    "\n",
    "    for car in carousels[:2]:  # SPEED: limit number of carousels parsed\n",
    "        try:\n",
    "            leaders.extend(parse_slide_html(car.get_attribute(\"innerHTML\") or \"\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        slides = []\n",
    "        for sel in slide_selectors:\n",
    "            try:\n",
    "                found = car.find_elements(By.CSS_SELECTOR, sel)\n",
    "                if found:\n",
    "                    slides = found\n",
    "                    break\n",
    "            except WebDriverException:\n",
    "                continue\n",
    "\n",
    "        next_btn = None\n",
    "        for sel in next_btn_selectors:\n",
    "            try:\n",
    "                btns = car.find_elements(By.CSS_SELECTOR, sel)\n",
    "                if btns:\n",
    "                    next_btn = btns[0]\n",
    "                    break\n",
    "            except WebDriverException:\n",
    "                continue\n",
    "\n",
    "        step = 0\n",
    "        while next_btn and step < MAX_CAROUSEL_STEPS:\n",
    "            if site_time_remaining() <= 0:\n",
    "                break\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "            except WebDriverException:\n",
    "                try:\n",
    "                    next_btn.click()\n",
    "                except Exception:\n",
    "                    break\n",
    "            time.sleep(0.4)\n",
    "            try:\n",
    "                leaders.extend(parse_slide_html(car.get_attribute(\"innerHTML\") or \"\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "            step += 1\n",
    "\n",
    "    uniq = {}\n",
    "    for L in leaders:\n",
    "        name = normalize_field(L.get(\"name\",\"\")).lower()\n",
    "        title = normalize_field(L.get(\"title\",\"\")).lower()\n",
    "        profile = normalize_field(L.get(\"profile_url\",\"\")).lower()\n",
    "        image = normalize_field(L.get(\"image_url\",\"\")).lower()\n",
    "        key = (name, title, profile, image)\n",
    "        if key not in uniq:\n",
    "            uniq[key] = L\n",
    "    return list(uniq.values())\n",
    "\n",
    "# ---------------------------\n",
    "# Generic extraction pipeline (Optimized + Adaptive enrichment)\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def extract_leaders_generic(driver, base_url: str) -> tuple[list[dict], str, dict]:\n",
    "    soup = get_soup(driver)\n",
    "    leaders = []\n",
    "    dbg = {\"extract_static\": {}}\n",
    "\n",
    "    # 1) JSON-LD + microdata (FAST PATH)\n",
    "    jsonld = parse_jsonld_people(soup, base_url)\n",
    "    leaders.extend(jsonld)\n",
    "    dbg[\"extract_static\"][\"jsonld_count\"] = len(jsonld)\n",
    "\n",
    "    micro_count = 0\n",
    "    for person in soup.select(\"[itemscope][itemtype*='Person']\"):\n",
    "        # existing microdata collection…\n",
    "        # (unchanged)\n",
    "        pass\n",
    "    dbg[\"extract_static\"][\"microdata_count\"] = micro_count\n",
    "\n",
    "    # Early exit if static sufficient\n",
    "    if leaders:\n",
    "        # existing de‑dup + validate…\n",
    "        pass\n",
    "\n",
    "    # 2) Sections/cards/headings/images (limit sections)\n",
    "    sections = find_leadership_sections(soup)\n",
    "    sec_cards = 0\n",
    "    for sec in sections[:4]:\n",
    "        c1 = extract_cards_from_container(sec, base_url)\n",
    "        c2 = harvest_from_images(sec, base_url)\n",
    "        c3 = harvest_name_title_from_headings(sec)\n",
    "        leaders.extend(c1); leaders.extend(c2); leaders.extend(c3)\n",
    "        sec_cards += len(c1) + len(c2) + len(c3)\n",
    "    dbg[\"extract_static\"][\"section_cards_count\"] = sec_cards\n",
    "\n",
    "    # >>> NEW: headings + nearby paragraphs (very effective on WordPress pages like CG Schmidt)\n",
    "    fast_pairs = extract_by_headings_nearby_title(soup)\n",
    "    leaders.extend(fast_pairs)\n",
    "    dbg[\"extract_static\"][\"fast_pairs\"] = len(fast_pairs)\n",
    "\n",
    "    # 3) Text-block fallback\n",
    "    text_fallback = extract_leaders_from_text_blocks(soup, base_url)\n",
    "    leaders.extend(text_fallback)\n",
    "    dbg[\"extract_static\"][\"text_block_pairs\"] = len(text_fallback)\n",
    "\n",
    "    # De‑dup + validate before heavy steps\n",
    "    uniq, dedup_static = {}, []\n",
    "    for L in leaders:\n",
    "        key = (\n",
    "            normalize_field(L.get(\"name\",\"\")).lower(),\n",
    "            normalize_field(L.get(\"title\",\"\")).lower(),\n",
    "            normalize_field(L.get(\"profile_url\",\"\")).lower()\n",
    "        )\n",
    "        if key not in uniq:\n",
    "            uniq[key] = L; dedup_static.append(L)\n",
    "\n",
    "    if dedup_static and validate_leaders(dedup_static):\n",
    "        dbg[\"pipeline\"] = \"leaders_found_static\"\n",
    "        return (dedup_static, \"leaders_found_static\", dbg)\n",
    "\n",
    "    # 4) Only try carousel if static found <4 leaders\n",
    "    if len(dedup_static) < 4:\n",
    "        leaders_carousel = extract_from_carousel(driver, base_url)\n",
    "        if leaders_carousel and validate_leaders(leaders_carousel):\n",
    "            dbg[\"pipeline\"] = \"leaders_found_carousel\"\n",
    "            return (leaders_carousel, \"leaders_found_carousel\", dbg)\n",
    "\n",
    "    # 5) Profile links crawl (ADAPTIVE), only if static <4\n",
    "    profile_links = []\n",
    "    if len(dedup_static) < 4:\n",
    "        for sec in sections:\n",
    "            profile_links.extend(collect_profile_links(sec, base_url))\n",
    "        profile_links = list(dict.fromkeys(profile_links))\n",
    "        dbg[\"profiles_collected\"] = len(profile_links)\n",
    "\n",
    "        if profile_links:\n",
    "            adaptive_max = MAX_PROFILES_TO_ENRICH\n",
    "            if len(dedup_static) < 2 and len(profile_links) >= 10:\n",
    "                adaptive_max = min(5, len(profile_links))\n",
    "\n",
    "            enriched = enrich_from_profiles(driver, profile_links, base_url, max_profiles=adaptive_max)\n",
    "            uniq2, out = {}, []\n",
    "            for L in enriched:\n",
    "                key = (\n",
    "                    normalize_field(L.get(\"name\",\"\")).lower(),\n",
    "                    normalize_field(L.get(\"title\",\"\")).lower(),\n",
    "                    normalize_field(L.get(\"profile_url\",\"\")).lower()\n",
    "                )\n",
    "                if key not in uniq2:\n",
    "                    uniq2[key] = L; out.append(L)\n",
    "            if out and validate_leaders(out):\n",
    "                dbg[\"pipeline\"] = \"leaders_found_via_profiles\"\n",
    "                return (out, \"leaders_found_via_profiles\", dbg)\n",
    "\n",
    "    dbg[\"pipeline\"] = \"leaders_not_listed_or_unrecognized_structure\"\n",
    "    return ([], \"leaders_not_listed_or_unrecognized_structure\", dbg)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# About extraction\n",
    "# ---------------------------\n",
    "\n",
    "def extract_about_from_soup(soup: BeautifulSoup) -> str:\n",
    "    heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\"]\n",
    "\n",
    "    def not_in_chrome(el):\n",
    "        p = el\n",
    "        while p and hasattr(p, \"parent\"):\n",
    "            if p.name in {\"header\", \"nav\", \"footer\", \"aside\"}:\n",
    "                return False\n",
    "            classes = p.get(\"class\", []) or []\n",
    "            if any(cls in {\"site-header\", \"global-nav\", \"site-footer\", \"footer\", \"navbar\", \"menu\"} for cls in classes):\n",
    "                return False\n",
    "            p = p.parent\n",
    "        return True\n",
    "\n",
    "    for el in soup.select(\"[id*='about'], [class*='about']\"):\n",
    "        text = clean_text(el.get_text(separator=\" \", strip=True))\n",
    "        if len(text) > 60 and not_in_chrome(el):\n",
    "            return text\n",
    "\n",
    "    for h in soup.find_all(heading_tags):\n",
    "        h_text = clean_text(h.get_text()).lower()\n",
    "        if any(k in h_text for k in ABOUT_KEYWORDS) and not_in_chrome(h):\n",
    "            section = h.find_parent([\"section\", \"article\", \"div\"]) or h\n",
    "            parts = [clean_text(h.get_text())]\n",
    "            for sib in section.find_all(recursive=False):\n",
    "                if sib == h:\n",
    "                    continue\n",
    "                if sib.name in heading_tags:\n",
    "                    break\n",
    "                parts.append(clean_text(sib.get_text(separator=\" \", strip=True)))\n",
    "            combined = clean_text(\" \".join(p for p in parts if p))\n",
    "            if len(combined) > 80:\n",
    "                return combined\n",
    "\n",
    "    for selector in [\"main\", \"[role='main']\", \"article\", \".content\", \".page-content\", \"#content\", \"#main\"]:\n",
    "        for el in soup.select(selector):\n",
    "            text = clean_text(el.get_text(separator=\" \", strip=True))\n",
    "            if \"about\" in text.lower() and len(text) > 120:\n",
    "                return text\n",
    "\n",
    "    COOKIE_TERMS = [\"cookie\", \"cookies\", \"opt out\", \"opt-out\", \"opt out of cookies\", \"save my preferences\", \"privacy\", \"consent\", \"preferences\", \"opt in\"]\n",
    "\n",
    "    def looks_like_cookie_text(t: str) -> bool:\n",
    "        tl = t.lower()\n",
    "        for term in COOKIE_TERMS:\n",
    "            if term in tl:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    paragraphs = [clean_text(p.get_text()) for p in soup.find_all(\"p\")]\n",
    "    # filter out cookie/privacy paragraphs\n",
    "    paragraphs = [p for p in paragraphs if p and not looks_like_cookie_text(p)]\n",
    "    candidates = [c for c in paragraphs if \"about\" in c.lower()]\n",
    "    if candidates:\n",
    "        return max(candidates, key=len)\n",
    "\n",
    "    # fallback: use body text but strip cookie/privacy lines first\n",
    "    full_lines = [l.strip() for l in re.split(r\"[\\n\\r]+|(?<=[\\.\\!\\?])\\s+\", clean_text(soup.get_text(separator=\" \\n\", strip=True))) if l.strip()]\n",
    "    filtered_lines = [l for l in full_lines if not looks_like_cookie_text(l)]\n",
    "    body_text = \" \".join(filtered_lines)\n",
    "    return body_text[:3000] if len(body_text) > 200 else body_text\n",
    "\n",
    "def _extract_article_cards_from_archive(soup: BeautifulSoup, base_url: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extract article 'cards' (title, href, date/excerpt if present) from a category/archive page.\n",
    "    We do NOT paginate; we only read what's on the first page.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    container = get_main_container(soup)\n",
    "\n",
    "    \n",
    "    selectors = [\n",
    "        \"article\",\n",
    "        \".wp-block-post\",\n",
    "        \".post\",\n",
    "        \".entry\",\n",
    "        \".card\",\n",
    "        \".grid-item\",\n",
    "        \".grid .item\",\n",
    "        \".news-item\",\n",
    "        \".insights-item\",\n",
    "        \".blog-item\"\n",
    "    ]\n",
    "\n",
    "    for sel in selectors:\n",
    "        for el in container.select(sel):\n",
    "            if site_time_remaining() <= 0:\n",
    "                return articles\n",
    "            # Link\n",
    "            a = el.find(\"a\", href=True)\n",
    "            href = urljoin(base_url, a[\"href\"]) if a else \"\"\n",
    "            if not href or href.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Title\n",
    "            title = \"\"\n",
    "            h = el.find([\"h1\",\"h2\",\"h3\",\"h4\"])\n",
    "            if h:\n",
    "                title = clean_text(h.get_text())\n",
    "            if not title and a:\n",
    "                title = clean_text(a.get_text())\n",
    "\n",
    "            # Date (best-effort from <time> or meta)\n",
    "            date_txt = \"\"\n",
    "            t = el.find(\"time\")\n",
    "            if t:\n",
    "                date_txt = clean_text(t.get(\"datetime\") or t.get_text() or \"\")\n",
    "            if not date_txt:\n",
    "                meta_date = el.find(attrs={\"class\": re.compile(r\"date|posted|time\", re.I)})\n",
    "                if meta_date:\n",
    "                    date_txt = clean_text(meta_date.get_text())\n",
    "\n",
    "            # Excerpt\n",
    "            excerpt = \"\"\n",
    "            for ex_sel in [\".excerpt\", \".entry-summary\", \".wp-block-post-excerpt\", \"p\"]:\n",
    "                ex_el = el.select_one(ex_sel)\n",
    "                if ex_el:\n",
    "                    excerpt = clean_text(ex_el.get_text())\n",
    "                    break\n",
    "\n",
    "            articles.append({\n",
    "                \"url\": href,\n",
    "                \"title\": title,\n",
    "                \"date\": date_txt,\n",
    "                \"excerpt\": excerpt\n",
    "            })\n",
    "\n",
    "            if len(articles) >= MAX_NEWS_ARTICLES:\n",
    "                return articles\n",
    "    \n",
    "# Fallback: find links that look like articles\n",
    "    if not articles:\n",
    "        for a in container.find_all(\"a\", href=True):\n",
    "            if site_time_remaining() <= 0:\n",
    "                break\n",
    "            href = urljoin(base_url, a[\"href\"])\n",
    "            if \"/news-insights/\" in href.lower() and href.rstrip(\"/\") != base_url.rstrip(\"/\"):\n",
    "                title = clean_text(a.get_text())\n",
    "                if len(title) >= 15:\n",
    "                    articles.append({\n",
    "                        \"url\": href,\n",
    "                        \"title\": title,\n",
    "                        \"date\": \"\",\n",
    "                        \"excerpt\": \"\"\n",
    "                    })\n",
    "            if len(articles) >= MAX_NEWS_ARTICLES:\n",
    "                break\n",
    "\n",
    "    return articles[:MAX_NEWS_ARTICLES]\n",
    "\n",
    "# ---------------------------\n",
    "# Leadership page selection (Optimized)\n",
    "# ---------------------------\n",
    "\n",
    "def probe_known_people_paths(base_url: str) -> list[str]:\n",
    "    base = base_url.rstrip(\"/\")\n",
    "    return [\n",
    "        urljoin(base + \"/\", \"people\"),\n",
    "        urljoin(base + \"/\", \"our-people\"),\n",
    "        urljoin(base + \"/\", \"eua-people\"),\n",
    "        urljoin(base + \"/\", \"team\"),\n",
    "        urljoin(base + \"/\", \"our-team\"),\n",
    "        urljoin(base + \"/\", \"staff\"),\n",
    "    ]\n",
    "\n",
    "def choose_leadership_page(driver, base_url: str, debug: dict) -> str | None:\n",
    "    strong = find_page_link_candidates(driver, STRONG_LEADERSHIP_KEYWORDS, strong=True)[:4]\n",
    "    weak   = find_page_link_candidates(driver, WEAK_LEADERSHIP_KEYWORDS, strong=False)[:3]\n",
    "    cand = strong + weak\n",
    "\n",
    "    debug[\"leadership_candidates\"] = [u for (u, _) in cand]\n",
    "    visited = set()\n",
    "\n",
    "    for u, _score in cand:\n",
    "        if u in visited:\n",
    "            continue\n",
    "        visited.add(u)\n",
    "        try:\n",
    "            ok = safe_get(driver, u)\n",
    "            if not ok:\n",
    "                debug.setdefault(\"errors\", []).append({\"url\": u, \"error\": \"timeout_or_cdp_block\"})\n",
    "                continue\n",
    "            wait_for_body(driver)\n",
    "            dismiss_cookie_banners(driver)\n",
    "            wait_for_content(driver, PERSON_CARD_SELECTORS + [\".slick-slide\", \".swiper-slide\", \".owl-item\"], timeout=6)\n",
    "            prime_page_for_extraction(driver)\n",
    "            deep_lazy_scroll(driver, cycles=1, pause=0.5)\n",
    "\n",
    "            cur = driver.current_url\n",
    "            parsed = urlparse(cur)\n",
    "            if (not (parsed.path or \"\").strip(\"/\")) or parsed.fragment:\n",
    "                debug.setdefault(\"rejected\", []).append({\"url\": cur, \"reason\": \"fragment_or_empty_path\"})\n",
    "                continue\n",
    "\n",
    "            soup = get_soup(driver)\n",
    "            path = parsed.path.lower()\n",
    "            if any(b in path for b in BAD_CONTEXT_PATHS):\n",
    "                debug.setdefault(\"rejected\", []).append({\"url\": cur, \"reason\": \"bad_context\"})\n",
    "                continue\n",
    "\n",
    "            # Quick accept: see if static extraction yields leaders\n",
    "            leaders, status_suffix, _dbg = extract_leaders_generic(driver, cur)\n",
    "            if leaders and validate_leaders(leaders):\n",
    "                debug[\"accepted\"] = cur\n",
    "                return cur\n",
    "            else:\n",
    "                debug.setdefault(\"rejected\", []).append({\"url\": cur, \"reason\": \"no_person_signals\"})\n",
    "        except Exception as e:\n",
    "            debug.setdefault(\"errors\", []).append({\"url\": u, \"error\": str(e)})\n",
    "            continue\n",
    "\n",
    "    # Try clicking keyword buttons on the base page\n",
    "    try:\n",
    "        ok = safe_get(driver, base_url)\n",
    "        if ok:\n",
    "            wait_for_body(driver); dismiss_cookie_banners(driver)\n",
    "            if click_keyword_button(driver, STRONG_LEADERSHIP_KEYWORDS + WEAK_LEADERSHIP_KEYWORDS):\n",
    "                wait_for_content(driver, PERSON_CARD_SELECTORS + [\".slick-slide\", \".swiper-slide\", \".owl-item\"], timeout=6)\n",
    "                prime_page_for_extraction(driver)\n",
    "                deep_lazy_scroll(driver, cycles=1, pause=0.5)\n",
    "\n",
    "                cur = driver.current_url\n",
    "                parsed = urlparse(cur)\n",
    "                if (not (parsed.path or \"\").strip(\"/\")) or parsed.fragment:\n",
    "                    debug.setdefault(\"rejected\", []).append({\"url\": cur, \"reason\": \"fragment_or_empty_path\"})\n",
    "                else:\n",
    "                    soup = get_soup(driver)\n",
    "                    leaders, status_suffix, _dbg = extract_leaders_generic(driver, cur)\n",
    "                    if leaders and validate_leaders(leaders):\n",
    "                        debug[\"accepted_via_button\"] = cur\n",
    "                        return cur\n",
    "                    else:\n",
    "                        debug.setdefault(\"rejected\", []).append({\"url\": cur, \"reason\": \"button_navigated_but_no_signals\"})\n",
    "    except Exception as e:\n",
    "        debug.setdefault(\"errors\", []).append({\"url\": base_url, \"error\": f\"button_click: {e}\"})\n",
    "\n",
    "    # Probe common slugs (fast)\n",
    "    for guess in probe_known_people_paths(base_url):\n",
    "        try:\n",
    "            ok = safe_get(driver, guess)\n",
    "            if not ok:\n",
    "                continue\n",
    "            wait_for_body(driver); dismiss_cookie_banners(driver)\n",
    "            wait_for_content(driver, PERSON_CARD_SELECTORS + [\".slick-slide\", \".swiper-slide\", \".owl-item\"], timeout=5)\n",
    "            prime_page_for_extraction(driver)\n",
    "\n",
    "            cur = driver.current_url\n",
    "            parsed = urlparse(cur)\n",
    "            if (not (parsed.path or \"\").strip(\"/\")) or parsed.fragment:\n",
    "                continue\n",
    "            soup = get_soup(driver)\n",
    "            path = parsed.path.lower()\n",
    "            if any(b in path for b in BAD_CONTEXT_PATHS):\n",
    "                continue\n",
    "\n",
    "            leaders, status_suffix, _dbg = extract_leaders_generic(driver, cur)\n",
    "            if leaders and validate_leaders(leaders):\n",
    "                debug[\"accepted_probe_known\"] = cur\n",
    "                return cur\n",
    "        except Exception as e:\n",
    "            debug.setdefault(\"errors\", []).append({\"url\": guess, \"error\": str(e)})\n",
    "            continue\n",
    "\n",
    "    # Fallback: None\n",
    "    return None\n",
    "\n",
    "def _extract_article_body(driver, article_url: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Load a single article URL and return (date, body_text).\n",
    "    - Prefer <article> or main container\n",
    "    - Trim menus/footers via get_main_container\n",
    "    - Cap to MAX_ARTICLE_BODY_CHARS\n",
    "    - Return date best-effort (if present on page)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ok = safe_get(driver, article_url)\n",
    "        if not ok:\n",
    "            return (\"\", \"\")\n",
    "        wait_for_body(driver)\n",
    "        dismiss_cookie_banners(driver)\n",
    "        soup = get_soup(driver)\n",
    "        container = soup.select_one(\"article\") or get_main_container(soup)\n",
    "\n",
    "        # Date (prefer <time datetime>)\n",
    "        date_txt = \"\"\n",
    "        t = container.find(\"time\") if container else None\n",
    "        if t:\n",
    "            date_txt = clean_text(t.get(\"datetime\") or t.get_text() or \"\")\n",
    "        if not date_txt and soup:\n",
    "            t2 = soup.find(\"time\")\n",
    "            if t2:\n",
    "                date_txt = clean_text(t2.get(\"datetime\") or t2.get_text() or \"\")\n",
    "\n",
    "        # Body: strip obvious chrome\n",
    "        if container:\n",
    "            # Remove common chrome sections within article\n",
    "            for bad in container.select(\"nav, aside, .share, .social, .tags, .post-meta, .wp-block-buttons\"):\n",
    "                bad.decompose()\n",
    "            body = clean_text(container.get_text(separator=\" \", strip=True))\n",
    "        else:\n",
    "            body = clean_text(soup.get_text(separator=\" \", strip=True))\n",
    "\n",
    "        body = body[:MAX_ARTICLE_BODY_CHARS]\n",
    "        return (date_txt, body)\n",
    "    except Exception:\n",
    "        return (\"\", \"\")\n",
    "\n",
    "# ---------------------------\n",
    "# Core: Process a single URL (Optimized with budget + UI expansion)\n",
    "# ---------------------------\n",
    "\n",
    "def process_url(driver, url: str) -> dict:\n",
    "    start_ts = time.time()\n",
    "    # set module-level deadline so helper functions can check remaining time\n",
    "    try:\n",
    "        global SITE_DEADLINE\n",
    "    except Exception:\n",
    "        pass\n",
    "    SITE_DEADLINE = start_ts + SITE_TIME_BUDGET\n",
    "    url = normalize_url(url)\n",
    "    result = {\n",
    "        \"url\": url,\n",
    "        \"about_url\": \"\",\n",
    "        \"about_text\": \"\",\n",
    "        \"leaders_url\": \"\",\n",
    "        \"leaders\": [],\n",
    "        \"leaders_summary\": \"\",\n",
    "        \"status\": \"ok\",\n",
    "        \"news_url\": \"\",\n",
    "        \"news_articles\": [],\n",
    "        \"debug\": {}\n",
    "    }\n",
    "\n",
    "    def time_remaining():\n",
    "        return SITE_TIME_BUDGET - (time.time() - start_ts)\n",
    "\n",
    "    try:\n",
    "        # LANDING\n",
    "        ok = safe_get(driver, url)\n",
    "        if not ok:\n",
    "            result[\"status\"] = \"timeout\"\n",
    "            return result\n",
    "        wait_for_body(driver)\n",
    "        dismiss_cookie_banners(driver)\n",
    "\n",
    "        # ABOUT (quick)\n",
    "        if time_remaining() <= 0:\n",
    "            result[\"status\"] = \"budget_exhausted\"\n",
    "            return result\n",
    "\n",
    "        about_link = find_about_link(driver)\n",
    "        if about_link:\n",
    "            ok = safe_get(driver, about_link)\n",
    "            if ok:\n",
    "                wait_for_body(driver); dismiss_cookie_banners(driver)\n",
    "                path = urlparse(driver.current_url).path.lower()\n",
    "                about_soup = get_soup(driver)\n",
    "            else:\n",
    "                about_soup = get_soup(driver)\n",
    "            if any(b in path for b in BAD_CONTEXT_PATHS) and path not in PREFERRED_ABOUT_PATHS:\n",
    "                result[\"about_text\"] = extract_about_from_soup(about_soup)\n",
    "                result[\"about_url\"] = \"\"\n",
    "            else:\n",
    "                result[\"about_url\"] = driver.current_url\n",
    "                result[\"about_text\"] = extract_about_from_soup(about_soup)\n",
    "        else:\n",
    "            about_soup = get_soup(driver)\n",
    "            result[\"about_text\"] = extract_about_from_soup(about_soup)\n",
    "            result[\"about_url\"] = \"\"\n",
    "\n",
    "        if time_remaining() <= 0:\n",
    "            result[\"status\"] = \"budget_exhausted\"\n",
    "            return result\n",
    "\n",
    "        # LEADERSHIP\n",
    "        # Use remaining budget efficiently—go straight to leadership discovery\n",
    "        ok = safe_get(driver, url)\n",
    "        if not ok:\n",
    "            result[\"status\"] = (result[\"status\"] + \"; landing_reload_timeout\").strip(\"; \")\n",
    "        wait_for_body(driver)\n",
    "        dismiss_cookie_banners(driver)\n",
    "\n",
    "        leaders_link = choose_leadership_page(driver, url, result[\"debug\"])\n",
    "        if leaders_link and time_remaining() > 0:\n",
    "            try:\n",
    "                # Enable images temporarily (optional; alt attributes are available without loading)\n",
    "                set_image_loading(driver, True)\n",
    "\n",
    "                ok = safe_get(driver, leaders_link)\n",
    "                if not ok:\n",
    "                    result[\"status\"] = (result[\"status\"] + \"; leaders_page_timeout\").strip(\"; \")\n",
    "                wait_for_body(driver); dismiss_cookie_banners(driver)\n",
    "                wait_for_content(driver, PERSON_CARD_SELECTORS + [\".slick-slide\", \".swiper-slide\", \".owl-item\"], timeout=6)\n",
    "\n",
    "                # NEW: expand tabs, load more, accordions, pagination\n",
    "                expand_leadership_ui(driver)\n",
    "\n",
    "                prime_page_for_extraction(driver)\n",
    "                deep_lazy_scroll(driver, cycles=2, pause=0.5)\n",
    "\n",
    "                parsed = urlparse(driver.current_url)\n",
    "                if (not (parsed.path or \"\").strip(\"/\")) or parsed.fragment:\n",
    "                    result[\"status\"] = (result[\"status\"] + \"; no_leaders_found\").strip(\"; \")\n",
    "                    result[\"leaders_url\"] = \"\"\n",
    "                    result[\"leaders\"] = []\n",
    "                    result[\"leaders_summary\"] = \"\"\n",
    "                else:\n",
    "                    result[\"leaders_url\"] = driver.current_url\n",
    "\n",
    "                    leaders, status_suffix, dbg = extract_leaders_generic(driver, result[\"leaders_url\"] or url)\n",
    "                    result[\"debug\"][\"extract\"] = dbg\n",
    "                    leaders = filter_to_likely_leaders(leaders, result[\"leaders_url\"] or url)\n",
    "                    leaders = dedup_leaders(leaders)\n",
    "\n",
    "                    if not validate_leaders(leaders):\n",
    "                        result[\"status\"] = (result[\"status\"] + \"; no_leaders_found\").strip(\"; \")\n",
    "                        result[\"leaders\"] = []\n",
    "                        result[\"leaders_summary\"] = \"\"\n",
    "                    else:\n",
    "                        result[\"leaders\"] = leaders\n",
    "                        result[\"leaders_summary\"] = \"; \".join(\n",
    "                            [f\"{normalize_field(L.get('name',''))} — {normalize_field(L.get('title',''))}\" for L in leaders if L.get(\"name\") or L.get(\"title\")]\n",
    "                        )\n",
    "                        result[\"status\"] = (result[\"status\"] + \"; \" + status_suffix).strip(\"; \")\n",
    "\n",
    "            except Exception as e:\n",
    "                result[\"status\"] = (result[\"status\"] + f\"; leaders_parse_error: {e}\").strip(\"; \")\n",
    "            finally:\n",
    "                # Disable images again for speed on next site/page\n",
    "                set_image_loading(driver, False)\n",
    "\n",
    "        else:\n",
    "            # No dedicated page found; try landing page once\n",
    "            leaders, status_suffix, dbg = extract_leaders_generic(driver, url)\n",
    "            result[\"debug\"][\"extract\"] = dbg\n",
    "            leaders = filter_to_likely_leaders(leaders, url)\n",
    "            leaders = dedup_leaders(leaders)\n",
    "\n",
    "            if not validate_leaders(leaders):\n",
    "                result[\"status\"] = (result[\"status\"] + \"; no_leaders_found; no_leadership_link_found\").strip(\"; \")\n",
    "                result[\"leaders\"] = []\n",
    "                result[\"leaders_summary\"] = \"\"\n",
    "            else:\n",
    "                result[\"leaders\"] = leaders\n",
    "                result[\"leaders_summary\"] = \"; \".join(\n",
    "                    [f\"{normalize_field(L.get('name',''))} — {normalize_field(L.get('title',''))}\" for L in leaders if L.get(\"name\") or L.get(\"title\")]\n",
    "                )\n",
    "                result[\"status\"] = (result[\"status\"] + \"; \" + status_suffix + \"; no_leadership_link_found\").strip(\"; \")\n",
    "    \n",
    "        # ---------------------------\n",
    "        # NEWS (first page only)\n",
    "        # ---------------------------\n",
    "        if time_remaining() <= 0:\n",
    "            result[\"status\"] = (result[\"status\"] + \"; budget_exhausted\").strip(\"; \")\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            ok = safe_get(driver, url)\n",
    "            if not ok:\n",
    "                result.setdefault(\"debug\", {}).setdefault(\"news_errors\", []).append({\"url\": url, \"error\": \"landing_timeout_for_news\"})\n",
    "            wait_for_body(driver); dismiss_cookie_banners(driver)\n",
    "            news_link = choose_news_page(driver, url, result[\"debug\"])\n",
    "            result[\"news_url\"] = news_link or \"\"\n",
    "\n",
    "            news_articles = []\n",
    "            if news_link and time_remaining() > 0:\n",
    "                ok = safe_get(driver, news_link)\n",
    "                if not ok:\n",
    "                    result.setdefault(\"debug\", {}).setdefault(\"news_errors\", []).append({\"url\": news_link, \"error\": \"news_page_timeout\"})\n",
    "                wait_for_body(driver); dismiss_cookie_banners(driver)\n",
    "                prime_page_for_extraction(driver)\n",
    "\n",
    "                soup_news = get_soup(driver)\n",
    "                cards = _extract_article_cards_from_archive(soup_news, news_link)\n",
    "                for card in cards:\n",
    "                    if time_remaining() <= 0:\n",
    "                        break\n",
    "                    # Avoid off-domain, fragments, PDFs\n",
    "                    parsed = urlparse(card[\"url\"])\n",
    "                    if parsed.scheme not in (\"http\",\"https\") or parsed.fragment or card[\"url\"].lower().endswith(\".pdf\"):\n",
    "                        continue\n",
    "                    # Fetch body\n",
    "                    dtxt, body = _extract_article_body(driver, card[\"url\"])\n",
    "                    article = {\n",
    "                        \"url\": card[\"url\"],\n",
    "                        \"title\": card[\"title\"],\n",
    "                        \"date\": dtxt or card[\"date\"],\n",
    "                        \"body\": body or card.get(\"excerpt\",\"\")\n",
    "                    }\n",
    "                    news_articles.append(article)\n",
    "                    if len(news_articles) >= MAX_NEWS_ARTICLES:\n",
    "                        break\n",
    "\n",
    "            result[\"news_articles\"] = news_articles\n",
    "        except Exception as e:\n",
    "            result.setdefault(\"debug\", {}).setdefault(\"news_errors\", []).append(str(e))\n",
    "            result[\"news_url\"] = \"\"\n",
    "            result[\"news_articles\"] = []\n",
    "\n",
    "    except TimeoutException:\n",
    "        result[\"status\"] = \"timeout\"\n",
    "    except WebDriverException as e:\n",
    "        result[\"status\"] = f\"webdriver_error: {e}\"\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = f\"error: {e}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------------------------\n",
    "# Runner (Optimized: reuse driver)\n",
    "# ---------------------------\n",
    "\n",
    "def run_all(URLS):\n",
    "    results = []\n",
    "    leader_rows = []\n",
    "    # Restart driver every N sites to avoid long-lived driver state/memory buildup\n",
    "    restart_every = globals().get('RESTART_DRIVER_EVERY', 10)\n",
    "    driver = None\n",
    "    try:\n",
    "        for idx, raw_url in enumerate(URLS):\n",
    "            # rebuild driver at chunk start\n",
    "            if driver is None:\n",
    "                driver = build_driver(headless=HEADLESS)\n",
    "\n",
    "            t0 = time.time()\n",
    "            print(f\"\\nProcessing: {raw_url}\")\n",
    "\n",
    "            try:\n",
    "                r = process_url(driver, raw_url)\n",
    "            finally:\n",
    "                try:\n",
    "                    driver.delete_all_cookies()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            leaders_summary = \"; \".join(\n",
    "                [f\"{normalize_field(L.get('name',''))} — {normalize_field(L.get('title',''))}\"\n",
    "                 for L in r.get(\"leaders\", []) if L.get(\"name\") or L.get(\"title\")]\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"url\": r.get(\"url\",\"\"),\n",
    "                \"about_url\": r.get(\"about_url\",\"\"),\n",
    "                \"about_text\": r.get(\"about_text\",\"\"),\n",
    "                \"leaders_url\": r.get(\"leaders_url\",\"\"),\n",
    "                \"leaders_summary\": leaders_summary,\n",
    "                \"status\": r.get(\"status\",\"\"),\n",
    "                \"news_url\": r.get(\"news_url\",\"\"),\n",
    "                \"news_res\" : r.get(\"news_articles\", \"\")\n",
    "            })\n",
    "\n",
    "            for L in r.get(\"leaders\", []):\n",
    "                leader_rows.append({\n",
    "                    \"site_url\": r.get(\"url\",\"\"),\n",
    "                    \"leaders_url\": r.get(\"leaders_url\",\"\"),\n",
    "                    \"name\": normalize_field(L.get(\"name\",\"\")),\n",
    "                    \"title\": normalize_field(L.get(\"title\",\"\")),\n",
    "                    \"profile_url\": normalize_field(L.get(\"profile_url\",\"\")),\n",
    "                    \"image_url\": normalize_field(L.get(\"image_url\",\"\")),\n",
    "                    \"bio\": normalize_field(L.get(\"bio\",\"\")),\n",
    "                })\n",
    "\n",
    "            t1 = time.time()\n",
    "            print(f\"Done: {raw_url} in {t1 - t0:.2f}s\")\n",
    "\n",
    "            # restart driver periodically\n",
    "            try:\n",
    "                if (idx + 1) % restart_every == 0:\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    driver = None\n",
    "            except Exception:\n",
    "                pass\n",
    "    finally:\n",
    "        if driver is not None:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return results, leader_rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93bf922c-78ac-4713-a761-76733a014f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "#    Predictor Code\n",
    "#----------------------------\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ---------- News scoring ----------\n",
    "NEWS_WEIGHT = 0.45          # news evidence is weaker than About\n",
    "NEWS_TOTAL_CAP = 2.5        # absolute cap on |z_news_own| + |z_news_op|\n",
    "\n",
    "# About/news family-term multiplier settings\n",
    "ABOUT_NEWS_MULT = 1.2       # fallback boost for news on About pages\n",
    "FAMILY_ARTICLE_MULT = 0.6   # per-article multiplier increment when family-term appears\n",
    "FAMILY_ARTICLE_MULT_CAP = 2.0\n",
    "FAMILY_ARTICLE_RX = re.compile(r\"\\bfamily\\b(?:\\W+\\w+){0,6}\\W+\\b(owned|operated|run|led)\\b\", re.I)\n",
    "MAX_NEWS_ARTICLES = 50\n",
    "PER_ARTICLE_CAP = 0.8       # cap contribution from any single article (tighter)\n",
    "DOMAIN_DEDUPE_FACTOR = 0.5  # weight for subsequent articles from same domain\n",
    "OWN_NEWS_MULT = 0.4         # reduce ownership-specific news weights slightly\n",
    "\n",
    "\n",
    "# ---------- Configurable priors ----------\n",
    "# Keep ownership low unless clear signals exist\n",
    "OWNERSHIP_PRIOR_Z = -2.2   # ~0.10 baseline with no evidence\n",
    "OPERATION_PRIOR_Z = -0.4   # unused when gate forces op=0, but applied after gate\n",
    "ROLE_CAP_TOTAL = 4.0\n",
    "BOARD_CAP_TOTAL = 1.2\n",
    "\n",
    "# ---------- News-only operation patterns (succession) ----------\n",
    "# Succession indicators remain strong\n",
    "NEWS_OP_POS = [\n",
    "    (re.compile(r'\\bsucceeds?\\s+(his|her|their)\\s+(father|mother)\\b', re.I), 3.0),\n",
    "    (re.compile(r'\\bson\\s+of\\s+the\\s+founder\\b', re.I), 2.8),\n",
    "    (re.compile(r'\\bdaughter\\s+of\\s+the\\s+founder\\b', re.I), 2.8),\n",
    "    (re.compile(r'\\bsecond[-\\s]?generation\\b', re.I), 2.2),\n",
    "    (re.compile(r'\\bthird[-\\s]?generation\\b', re.I), 2.4),\n",
    "    (re.compile(r'\\bfamily\\s+legacy\\b', re.I), 1.6),\n",
    "    # Broader signals indicating succession/lineage or leadership mentions (reduced weight)\n",
    "    (re.compile(r'\\b(named|promoted|appointed|elected)\\b', re.I), 0.15),\n",
    "    (re.compile(r'\\b(nominated|honor(ed|s)?|awarded)\\b', re.I), 0.15),\n",
    "]\n",
    "\n",
    "# ---------- Ownership-positive patterns ----------\n",
    "OWN_POS = [\n",
    "    (re.compile(r'\\bfamily[-\\s]?owned\\b', re.I), 3.2),\n",
    "    (re.compile(r'\\bfamily[-\\s]?business\\b', re.I), 2.0),\n",
    "    (re.compile(r'\\bfamily\\s+member\\s+since\\b', re.I), 2.0),\n",
    "    (re.compile(r'\\bin\\s+family\\s+ownership\\b', re.I), 2.6),\n",
    "    (re.compile(r'\\bthe\\s+([A-Z][a-z]+)\\s+family\\b'), 2.2),\n",
    "    (re.compile(r'\\b(?:third|fourth|fifth|multi)[-\\s]?generation\\b', re.I), 2.4),\n",
    "    (re.compile(r'\\bprivately\\s+owned\\s+by\\s+the\\s+[A-Z][a-z]+\\s+family\\b', re.I), 2.8),\n",
    "    # Co-occurrence: \"family\" near \"owned\"\n",
    "    (re.compile(r'\\bfamily\\b(?:\\W+\\w+){0,6}\\W+\\bowned\\b', re.I), 2.6),\n",
    "    # Spelled-out / numeric \"X generations\"\n",
    "    (re.compile(r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten)\\s+generations?\\b', re.I), 2.2),\n",
    "    (re.compile(r'\\b\\d{1,2}\\s+generations?\\b', re.I), 2.2),\n",
    "    # Broader ownership-ish mentions sometimes found in press/company history\n",
    "    (re.compile(r'\\bcentury[-\\s]?old|five\\s+generations|family\\s+legacy\\b', re.I), 1.6),\n",
    "]\n",
    "\n",
    "# ---------- Operation-positive patterns (added only after gate passes) ----------\n",
    "OP_POS = [\n",
    "    (re.compile(r'\\bfamily[-\\s]?operated\\b', re.I), 3.0),\n",
    "    (re.compile(r'\\bfamily\\s+member\\s+since\\b', re.I), 2.0),\n",
    "    (re.compile(r'\\bfamily[-\\s]?run\\b', re.I), 2.8),\n",
    "    (re.compile(r'\\bower[-\\s]?operated\\b', re.I), 2.2),\n",
    "    (re.compile(r'\\bfounder[-\\s]?led\\b', re.I), 1.4),\n",
    "    (re.compile(r'\\bfamily\\b(?:\\W+\\w+){0,6}\\W+\\b(operated|run|led)\\b', re.I), 2.6),\n",
    "    # Broader leadership/appointment language — lowered weight\n",
    "    (re.compile(r'\\b(promoted|appointed|named|served\\s+as)\\b', re.I), 0.15),\n",
    "    # Named ... title pattern: reduce weight and will require title proximity check\n",
    "    (re.compile(r'\\bnamed\\b[\\s\\S]{0,120}?\\b(president|ceo|chair|vice president|director|owner)\\b', re.I), 0.25),\n",
    "]\n",
    "\n",
    "# ---------- Negatives (private-firm relevant; your list preserved) ----------\n",
    "NEG = [\n",
    "    (re.compile(r'\\bsubsidiary\\s+of\\b', re.I), -2.6),\n",
    "    (re.compile(r'\\bdivision\\s+of\\b', re.I), -2.2),\n",
    "    (re.compile(r'\\bacquired\\s+by\\b', re.I), -2.2),\n",
    "    (re.compile(r'\\bwholly\\s+owned\\s+by\\b', re.I), -2.6),\n",
    "    (re.compile(r'\\bportfolio\\s+company\\b', re.I), -2.5),\n",
    "    (re.compile(r'\\bpart\\s+of\\b', re.I), -1.6),\n",
    "    (re.compile(r'\\bmanaged\\s+by\\b', re.I), -1.4),\n",
    "    (re.compile(r'\\bfranchisee\\s+of\\b', re.I), -1.0),\n",
    "]\n",
    "\n",
    "# ---------- Anti false-positive ----------\n",
    "ANTI_FP = [\n",
    "    re.compile(r'customers?\\s+like\\s+family', re.I),\n",
    "    re.compile(r'join\\s+our\\s+family', re.I),\n",
    "    re.compile(r'\\bfamily\\s+values\\b', re.I),\n",
    "]\n",
    "\n",
    "# ---------- Leadership URL keywords ----------\n",
    "LEADERSHIP_URL_KEYWORDS = [\"leadership\", \"people\", \"team\", \"management\", \"executives\", \"owners\"]\n",
    "\n",
    "# ---------- Token-based role matching ----------\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def parse_date_best_effort(dt_str: str) -> datetime | None:\n",
    "    \"\"\"\n",
    "    Accepts ISO-like or textual month formats; returns naive UTC datetime or None.\n",
    "    We avoid external libraries; handle 'YYYY-MM-DD', 'Month DD, YYYY', etc.\n",
    "    \"\"\"\n",
    "    s = (dt_str or \"\").strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # ISO first\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%d %H:%M\", \"%Y/%m/%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Textual month\n",
    "    for fmt in (\"%b %d, %Y\", \"%B %d, %Y\", \"%d %b %Y\", \"%d %B %Y\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def news_recency_factor(dt: datetime | None) -> float:\n",
    "    \"\"\"\n",
    "    Scale contribution by age: <= 730 days -> 1.0; else 0.6.\n",
    "    \"\"\"\n",
    "    if not dt:\n",
    "        return 1.0\n",
    "    try:\n",
    "        now = datetime.now(timezone.utc).replace(tzinfo=None)\n",
    "        age_days = max(0, (now - dt).days)\n",
    "        return 1.0 if age_days <= 730 else 0.6\n",
    "    except Exception:\n",
    "        return 1.0\n",
    "\n",
    "def extract_all_person_names_from_news(articles: List[Dict]) -> List[str]:\n",
    "    names = []\n",
    "    rx_name = re.compile(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b\")  # simple proper case\n",
    "    for a in articles:\n",
    "        text = \" \".join([a.get(\"title\",\"\"), a.get(\"body\",\"\")])\n",
    "        for m in rx_name.finditer(text):\n",
    "            cand = m.group(1).strip()\n",
    "            if is_valid_person_name(cand):\n",
    "                names.append(cand)\n",
    "    return names\n",
    "\n",
    "def classify_news_page(articles: List[Dict]) -> str:\n",
    "    names = extract_all_person_names_from_news(articles)\n",
    "    surnames = [last_name_of(n) for n in names if n]\n",
    "    repeated = 0\n",
    "    if surnames:\n",
    "        for s in set(surnames):\n",
    "            repeated = max(repeated, surnames.count(s))\n",
    "    lineage_terms = 0\n",
    "    joined = \" \".join([a.get(\"title\",\"\") + \" \" + a.get(\"body\",\"\") for a in articles])\n",
    "    for kw in [\"founded\", \"since\", \"generation\", \"family\"]:\n",
    "        if re.search(rf\"\\b{kw}\\b\", joined, re.I):\n",
    "            lineage_terms += 1\n",
    "    # Make lineage detection stricter: require multiple lineage keywords, or strong repeated-name signal plus a lineage keyword\n",
    "    if lineage_terms >= 2 or (lineage_terms >= 1 and repeated >= 3):\n",
    "        return \"lineage\"\n",
    "    return \"corporate\"\n",
    "\n",
    "def norm_role(role_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize role to lowercase tokens, keeping alnum, spaces, hyphen, plus.\n",
    "    \"\"\"\n",
    "    r = role_text or \"\"\n",
    "    r = r.lower()\n",
    "    r = re.sub(r'[^a-z0-9\\s\\-+]', ' ', r)  # remove punctuation except hyphen/plus\n",
    "    r = re.sub(r'\\s+', ' ', r).strip()\n",
    "    return r\n",
    "\n",
    "# Exact tokens (standalone words) -> weights\n",
    "ROLE_TOKENS: Dict[str, float] = {\n",
    "    'owner': 2.2, 'co-owner': 2.0,\n",
    "    'founder': 1.5, 'co-founder': 1.4,\n",
    "    'president': 1.8, 'vice president' : 1.0,\n",
    "    'ceo': 1.6, 'coo': 1.2, 'cfo': 0.9,   # acronyms as tokens only\n",
    "    'principal': 1.0, 'partner': 1.0,\n",
    "    'chairman': 0.8, 'chair': 0.8,\n",
    "    'director': 0.5,                      # generic director modest\n",
    "}\n",
    "\n",
    "# Exact phrases (contiguous substrings) -> weights\n",
    "ROLE_PHRASES: Dict[str, float] = {\n",
    "    'vice president': 1.0,\n",
    "    'chief executive officer': 1.6,\n",
    "    'chief operating officer': 1.2,\n",
    "    'chief financial officer': 0.9,\n",
    "    'managing director': 1.5,\n",
    "    'general manager': 1.4,\n",
    "    'board member': 0.4,\n",
    "}\n",
    "\n",
    "# Blocklist: not leadership for clustering/weights\n",
    "ROLE_BLOCKLIST = {'coordinator', 'assistant', 'specialist', 'administrator', 'ambassador', 'intern'}\n",
    "\n",
    "def role_weight(role_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Return leadership weight for a role using tokens/phrases (0 if blocklisted or no match).\n",
    "    \"\"\"\n",
    "    r = norm_role(role_text)\n",
    "    tokens = set(r.split())\n",
    "    # Blocklist\n",
    "    if any(b in tokens for b in ROLE_BLOCKLIST):\n",
    "        return 0.0\n",
    "    # Phrase hits first (prefer multi-word exact phrases like 'vice president')\n",
    "    # iterate phrases in descending length to prefer longer matches\n",
    "    for phrase, w in sorted(ROLE_PHRASES.items(), key=lambda kv: -len(kv[0])):\n",
    "        if phrase in r:\n",
    "            return w\n",
    "    # Token hits (single-word tokens only) — skip multi-word keys here\n",
    "    for tok, w in ROLE_TOKENS.items():\n",
    "        if ' ' in tok:\n",
    "            continue\n",
    "        if tok in tokens:\n",
    "            return w\n",
    "    return 0.0\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def sigmoid(z: float) -> float:\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    return (t or \"\").strip()\n",
    "\n",
    "def parse_leaders(summary: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Split 'Name — Role; Name — Role; ...' into list of (name, role).\"\"\"\n",
    "    if not summary:\n",
    "        return []\n",
    "    entries = [e.strip() for e in re.split(r';\\s*', summary) if e.strip()]\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    for e in entries:\n",
    "        parts = re.split(r'\\s+[—\\-–]\\s+', e)   # em dash / en dash / hyphen\n",
    "        if len(parts) == 1:\n",
    "            parts = re.split(r'\\s*,\\s*', e, maxsplit=1)  # fallback on comma\n",
    "        name = parts[0].strip()\n",
    "        role = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        pairs.append((name, role))\n",
    "    return pairs\n",
    "\n",
    "def is_valid_person_name(name: str) -> bool:\n",
    "    if not name or re.search(r'\\d', name):\n",
    "        return False\n",
    "    tmp = re.sub(r'[()&]', '', name).strip()\n",
    "    tokens = tmp.split()\n",
    "    if len(tokens) < 2:\n",
    "        return False\n",
    "    return tokens[0][0].isupper() and tokens[1][0].isupper()\n",
    "\n",
    "def last_name_of(name: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z\\-]', '', (name.split()[-1] if name else \"\")).lower()\n",
    "\n",
    "# ---------- Leadership-only weighted surname clustering ----------\n",
    "def leadership_cluster(leaders_list: List[Tuple[str, str]]) -> Tuple[float, str, Dict[str, float], float, int, float, bool]:\n",
    "    \"\"\"\n",
    "    Compute weighted surname clustering among leadership-only.\n",
    "    Includes only entries with role_weight > 0.\n",
    "    Returns: (ratio_weighted, top_surname, weighted_counts, total_weight, shared_count_top, max_role_weight_top, family_role_present)\n",
    "    \"\"\"\n",
    "    counts_w: Dict[str, float] = {}\n",
    "    counts_raw: Dict[str, int] = {}\n",
    "    max_role_w_by_ln: Dict[str, float] = {}\n",
    "    total_w = 0.0\n",
    "    family_role_present = False\n",
    "\n",
    "    for name, role in leaders_list:\n",
    "        if not is_valid_person_name(name):\n",
    "            continue\n",
    "        w = role_weight(role)\n",
    "        # If the role text explicitly mentions 'family' or 'family member', treat it as stronger leadership-family signal\n",
    "        try:\n",
    "            if role and re.search(r'family\\s+member|family', role, re.I):\n",
    "                family_role_present = True\n",
    "                w = w * 1.6\n",
    "        except Exception:\n",
    "            pass\n",
    "        if w <= 0:\n",
    "            continue\n",
    "        ln = last_name_of(name)\n",
    "        if not ln:\n",
    "            continue\n",
    "        counts_w[ln] = counts_w.get(ln, 0.0) + w\n",
    "        counts_raw[ln] = counts_raw.get(ln, 0) + 1\n",
    "        max_role_w_by_ln[ln] = max(max_role_w_by_ln.get(ln, 0.0), w)\n",
    "        total_w += w\n",
    "\n",
    "    if total_w <= 0 or not counts_w:\n",
    "        return (0.0, \"\", counts_w, total_w, 0, 0.0, family_role_present)\n",
    "\n",
    "    top_ln, top_w = max(counts_w.items(), key=lambda kv: kv[1])\n",
    "    ratio = top_w / total_w\n",
    "    shared_count_top = counts_raw.get(top_ln, 0)\n",
    "    max_role_w_top = max_role_w_by_ln.get(top_ln, 0.0)\n",
    "    return (ratio, top_ln, counts_w, total_w, shared_count_top, max_role_w_top, family_role_present)\n",
    "\n",
    "# ---------- Main scorer ----------\n",
    "\n",
    "def score_news_articles(articles: List[Dict], page_type: str, leader_full_set: set | None = None) -> Tuple[float, float, List[Tuple[str, float]], List[Tuple[str, float]], List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Return (z_news_own, z_news_op, evidence_own, evidence_op, evidence_neg)\n",
    "    Applies NEWS_WEIGHT and recency factor per-article.\n",
    "    Caps total contribution.\n",
    "    Implements stricter proximity and corroboration for generic matches.\n",
    "    Now requires that articles provide a leader surname mention and a family-term before counting appointment/award matches.\n",
    "    Adds per-article caps and per-domain deduplication to avoid press-kit inflation.\n",
    "    Ownership news now requires corroboration across >=2 distinct domains to count.\n",
    "    \"\"\"\n",
    "    z_no, z_np = 0.0, 0.0\n",
    "    evidence_own: List[Tuple[str, float]] = []\n",
    "    evidence_op:  List[Tuple[str, float]] = []\n",
    "    evidence_neg: List[Tuple[str, float]] = []\n",
    "\n",
    "    generic_entries: List[Tuple[int, float, str, float]] = []  # (article_idx, w_eff, snippet, domain_factor)\n",
    "    seen_domains: Dict[str, int] = {}\n",
    "\n",
    "    # ownership accumulation by domain (deferred credit)\n",
    "    ownership_by_domain: Dict[str, float] = {}\n",
    "    ownership_snips_by_domain: Dict[str, List[str]] = {}\n",
    "\n",
    "    # derive leader surnames from leader_full_set (if provided)\n",
    "    leader_surnames = set()\n",
    "    try:\n",
    "        if leader_full_set:\n",
    "            for n in leader_full_set:\n",
    "                if not n:\n",
    "                    continue\n",
    "                # leader_full_set entries are expected as 'first last' lowercase; extract last token\n",
    "                leader_surnames.add(last_name_of(n))\n",
    "    except Exception:\n",
    "        leader_surnames = set()\n",
    "\n",
    "    for ai, a in enumerate((articles or [])[:MAX_NEWS_ARTICLES]):\n",
    "        text = (a.get(\"body\",\"\") or \"\") + \" \" + (a.get(\"title\",\"\") or \"\")\n",
    "        dt = parse_date_best_effort(a.get(\"date\",\"\"))\n",
    "        rec = news_recency_factor(dt)\n",
    "\n",
    "        # domain dedupe factor\n",
    "        domain = \"\"\n",
    "        try:\n",
    "            url = (a.get(\"url\") or a.get(\"source\") or \"\")\n",
    "            m = re.search(r'https?://([^/]+)', url)\n",
    "            if m:\n",
    "                domain = m.group(1).lower()\n",
    "        except Exception:\n",
    "            domain = \"\"\n",
    "        seen_count = seen_domains.get(domain, 0)\n",
    "        domain_factor = 1.0 if seen_count == 0 else DOMAIN_DEDUPE_FACTOR\n",
    "        seen_domains[domain] = seen_count + 1\n",
    "\n",
    "        # per-article accumulators\n",
    "        per_article_z_no = 0.0\n",
    "        per_article_z_np = 0.0\n",
    "        per_article_own_snips: List[str] = []\n",
    "\n",
    "        # determine if this article explicitly mentions any leader surname\n",
    "        has_leader_surname = True if not leader_surnames else False\n",
    "        if leader_surnames:\n",
    "            for ln in leader_surnames:\n",
    "                try:\n",
    "                    if re.search(r\"\\b\" + re.escape(ln) + r\"\\b\", text, re.I):\n",
    "                        has_leader_surname = True\n",
    "                        break\n",
    "                except re.error:\n",
    "                    continue\n",
    "\n",
    "        # require family-term anywhere in article for per-article qualification\n",
    "        family_terms = (\"family\", \"founder\", \"generation\", \"son\", \"daughter\", \"succeeds\", \"legacy\", \"succession\", \"founding\")\n",
    "        has_family_term = False\n",
    "        for term in family_terms:\n",
    "            try:\n",
    "                if re.search(r\"\\b\" + re.escape(term) + r\"\\b\", text, re.I):\n",
    "                    has_family_term = True\n",
    "                    break\n",
    "            except re.error:\n",
    "                continue\n",
    "\n",
    "        # normalize article person full-names (first + last)\n",
    "        def _norm_full(name: str) -> str:\n",
    "            if not name:\n",
    "                return \"\"\n",
    "            toks = [t for t in re.sub(r\"[^A-Za-z\\\\s]\", \" \", name).split() if t]\n",
    "            if len(toks) >= 2:\n",
    "                return f\"{toks[0].lower()} {toks[-1].lower()}\"\n",
    "            return \"\"\n",
    "        article_names = [ _norm_full(n) for n in extract_all_person_names_from_news([a]) ]\n",
    "        article_full_set = {n for n in article_names if n}\n",
    "\n",
    "        # helper: leader proximity and family-term proximity\n",
    "        def _leader_near_match(span_start, span_end, leaders, window=200):\n",
    "            if not leaders:\n",
    "                return False\n",
    "            for leader in leaders:\n",
    "                try:\n",
    "                    for lm in re.finditer(r\"\\\\b\" + re.escape(leader) + r\"\\\\b\", text.lower()):\n",
    "                        if lm.start() >= max(0, span_start - window) and lm.start() <= span_end + window:\n",
    "                            return True\n",
    "                except re.error:\n",
    "                    continue\n",
    "            return False\n",
    "        def _term_near_match(span_start, span_end, terms, window=200):\n",
    "            if not terms:\n",
    "                return False\n",
    "            for term in terms:\n",
    "                try:\n",
    "                    for tm in re.finditer(r\"\\\\b\" + re.escape(term) + r\"\\\\b\", text, re.I):\n",
    "                        if tm.start() >= max(0, span_start - window) and tm.start() <= span_end + window:\n",
    "                            return True\n",
    "                except re.error:\n",
    "                    continue\n",
    "            return False\n",
    "\n",
    "        # optional debug: show article preview and which patterns match\n",
    "        if False:\n",
    "            try:\n",
    "                print(\"ARTICLE PREVIEW:\", text[:300])\n",
    "                for rx, w in OWN_POS + OP_POS + NEWS_OP_POS + NEG:\n",
    "                    m = rx.search(text)\n",
    "                    if m:\n",
    "                        print(\"MATCH:\", rx.pattern, \"->\", (m.group(0) or \"\"), \"w=\", w)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Ownership positives (always considered) — reduced by OWN_NEWS_MULT and accumulated per-article\n",
    "        for rx, w in OWN_POS:\n",
    "            for m in rx.finditer(text):\n",
    "                w_eff = w * NEWS_WEIGHT * OWN_NEWS_MULT * rec\n",
    "                per_article_z_no += w_eff\n",
    "                snippet = text[max(0, m.start()-60): m.end()+60]\n",
    "                per_article_own_snips.append(snippet.strip())\n",
    "\n",
    "        # If article does not mention a leader surname or lacks family-term, skip operation/appointment patterns to avoid false positives\n",
    "        if not (has_leader_surname and has_family_term):\n",
    "            # defer ownership credit to domain aggregation (store per-domain)\n",
    "            adj_own = min(per_article_z_no, PER_ARTICLE_CAP) * domain_factor\n",
    "            ownership_by_domain[domain] = ownership_by_domain.get(domain, 0.0) + adj_own\n",
    "            if per_article_own_snips:\n",
    "                ownership_snips_by_domain.setdefault(domain, []).extend(per_article_own_snips)\n",
    "            continue\n",
    "\n",
    "        # Operation positives (generic) — only processed if article mentions leader surname and family-term\n",
    "        generic_keywords = (\"named\", \"promoted\", \"appointed\", \"served\", \"nominated\", \"honor\", \"awarded\", \"elected\")\n",
    "        for rx, w in OP_POS:\n",
    "            for m in rx.finditer(text):\n",
    "                is_generic = any(k in rx.pattern for k in generic_keywords)\n",
    "                w_eff = w * NEWS_WEIGHT * rec\n",
    "                snippet = text[max(0, m.start()-60): m.end()+60]\n",
    "\n",
    "                # For generic appointment/award patterns, require full-name proximity and family-term proximity\n",
    "                if is_generic and page_type != \"lineage\" and leader_full_set:\n",
    "                    if not (_leader_near_match(m.start(), m.end(), leader_full_set) and _term_near_match(m.start(), m.end(), family_terms)):\n",
    "                        continue\n",
    "                    # If this is specifically the named...title pattern, require a title token nearby (tighten false positives)\n",
    "                    if 'president' in rx.pattern or 'ceo' in rx.pattern or 'chair' in rx.pattern or 'vice president' in rx.pattern or 'owner' in rx.pattern:\n",
    "                        window_snip = text[max(0, m.start()-120): m.end()+120]\n",
    "                        if not re.search(r\"\\b(president|ceo|chair|owner)\\b\", window_snip, re.I):\n",
    "                            continue\n",
    "                    # Defer corroboration: store generic entries for post-article aggregation (include domain factor)\n",
    "                    generic_entries.append((ai, w_eff, snippet.strip(), domain_factor))\n",
    "                    per_article_z_np += w_eff\n",
    "                    continue\n",
    "                # Non-generic or lineage-allowed -> immediate credit (accumulate per-article)\n",
    "                per_article_z_np += w_eff\n",
    "                evidence_op.append((snippet.strip(), round(w_eff,3)))\n",
    "\n",
    "        # News-only operation (succession)\n",
    "        for rx, w in NEWS_OP_POS:\n",
    "            for m in rx.finditer(text):\n",
    "                succession_keywords = (\"succeeds\", \"son\", \"daughter\", \"generation\")\n",
    "                is_succession = any(k in rx.pattern for k in succession_keywords)\n",
    "                w_eff = w * NEWS_WEIGHT * rec\n",
    "                snippet = text[max(0, m.start()-60): m.end()+60]\n",
    "\n",
    "                if not is_succession and page_type != \"lineage\" and leader_full_set:\n",
    "                    if not (_leader_near_match(m.start(), m.end(), leader_full_set) and _term_near_match(m.start(), m.end(), family_terms)):\n",
    "                        continue\n",
    "                    # treat broader NEWS_OP generic matches similarly (defer for corroboration)\n",
    "                    generic_entries.append((ai, w_eff, snippet.strip(), domain_factor))\n",
    "                    per_article_z_np += w_eff\n",
    "                    continue\n",
    "                per_article_z_np += w_eff\n",
    "                evidence_op.append((snippet.strip(), round(w_eff,3)))\n",
    "\n",
    "        # Negatives (still processed even when leader surname present)\n",
    "        for rx, w in NEG:\n",
    "            for m in rx.finditer(text):\n",
    "                w_eff = w * NEWS_WEIGHT * rec\n",
    "                per_article_z_no += w_eff\n",
    "                per_article_z_np += w_eff\n",
    "                snippet = text[max(0, m.start()-60): m.end()+60]\n",
    "                evidence_neg.append((snippet.strip(), round(w_eff,3)))\n",
    "\n",
    "        # apply per-article caps and domain factor after processing this article\n",
    "        adj_own = min(per_article_z_no, PER_ARTICLE_CAP) * domain_factor\n",
    "        ownership_by_domain[domain] = ownership_by_domain.get(domain, 0.0) + adj_own\n",
    "        if per_article_own_snips:\n",
    "            ownership_snips_by_domain.setdefault(domain, []).extend(per_article_own_snips)\n",
    "\n",
    "        z_np += min(per_article_z_np, PER_ARTICLE_CAP) * domain_factor\n",
    "\n",
    "    # Post-process ownership_by_domain: require corroboration across >=2 distinct domains to count ownership news credit\n",
    "    distinct_domains_with_own = [d for d, v in ownership_by_domain.items() if v > 0]\n",
    "    if len(distinct_domains_with_own) >= 2:\n",
    "        total_own_news = sum(ownership_by_domain.values())\n",
    "        z_no += total_own_news\n",
    "        # add evidence per-domain\n",
    "        for d in distinct_domains_with_own:\n",
    "            snips = ownership_snips_by_domain.get(d, [])\n",
    "            if snips:\n",
    "                evidence_own.append((f\"news(ownership, {d}): \" + \" ; \".join(snips), round(ownership_by_domain.get(d,0.0),3)))\n",
    "    else:\n",
    "        # no corroboration across domains -> drop news ownership credit\n",
    "        pass\n",
    "\n",
    "    # Post-process generic entries: require corroboration across >=2 distinct articles to give full credit,\n",
    "    # otherwise drop single-article generic signals to reduce false positives.\n",
    "    if generic_entries:\n",
    "        # group by article index\n",
    "        per_article_generic: Dict[int, float] = {}\n",
    "        per_article_domain_factor: Dict[int, float] = {}\n",
    "        per_article_snips: Dict[int, List[str]] = {}\n",
    "        for ai, w_eff, snippet, df in generic_entries:\n",
    "            per_article_generic[ai] = per_article_generic.get(ai, 0.0) + w_eff\n",
    "            per_article_domain_factor[ai] = df\n",
    "            per_article_snips.setdefault(ai, []).append(snippet)\n",
    "\n",
    "        article_set = set(per_article_generic.keys())\n",
    "        if len(article_set) >= 2:\n",
    "            for ai in sorted(article_set):\n",
    "                summed = per_article_generic.get(ai, 0.0)\n",
    "                df = per_article_domain_factor.get(ai, 1.0)\n",
    "                adj = min(summed, PER_ARTICLE_CAP) * df\n",
    "                z_np += adj\n",
    "                # join snippets for evidence\n",
    "                evidence_op.append((\"news(generic): \" + \" ; \".join(per_article_snips.get(ai, [])), round(adj,3)))\n",
    "\n",
    "    # Collapse duplicate snippets within each evidence list (sum weights)\n",
    "    def _collapse(evidence: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "        acc: Dict[str, float] = {}\n",
    "        order: List[str] = []\n",
    "        for s, w in evidence:\n",
    "            if s not in acc:\n",
    "                order.append(s)\n",
    "                acc[s] = 0.0\n",
    "            acc[s] += float(w)\n",
    "        return [(s, round(acc[s], 3)) for s in order]\n",
    "\n",
    "    evidence_own = _collapse(evidence_own)\n",
    "    evidence_op  = _collapse(evidence_op)\n",
    "    evidence_neg = _collapse(evidence_neg)\n",
    "\n",
    "    # Page-type caps\n",
    "    if page_type == \"corporate\":\n",
    "        # allow only small ownership nudge, roles ok but capped\n",
    "        z_no = max(min(z_no, 0.3), -NEWS_TOTAL_CAP)\n",
    "        z_np = max(min(z_np, 1.2), -NEWS_TOTAL_CAP)\n",
    "    else:\n",
    "        # lineage page: full cap\n",
    "        z_no = max(min(z_no, NEWS_TOTAL_CAP), -NEWS_TOTAL_CAP)\n",
    "        z_np = max(min(z_np, NEWS_TOTAL_CAP), -NEWS_TOTAL_CAP)\n",
    "\n",
    "    return (z_no, z_np, evidence_own, evidence_op, evidence_neg)\n",
    "\n",
    "\n",
    "def score_record(rec: Dict) -> Dict:\n",
    "    url = rec.get(\"url\")\n",
    "    about_url = rec.get(\"about_url\", \"\") or \"\"\n",
    "    leaders_url = rec.get(\"leaders_url\", \"\") or \"\"\n",
    "    about_text = clean_text(rec.get(\"about_text\", \"\"))\n",
    "    leaders_summary = clean_text(rec.get(\"leaders_summary\", \"\"))\n",
    "    # Parsed leaders (used for later name cross-matching with news)\n",
    "    leaders_for_news = parse_leaders(leaders_summary) if leaders_summary else []\n",
    "\n",
    "    # ---- News fields from record (available even if empty) ----\n",
    "    news_articles = rec.get(\"news_res\", []) or []\n",
    "    news_url = rec.get(\"news_url\", \"\") or \"\"\n",
    "\n",
    "    # Priors\n",
    "    z_own = OWNERSHIP_PRIOR_Z\n",
    "    z_op  = OPERATION_PRIOR_Z\n",
    "\n",
    "    evidence_own: List[Tuple[str, float]] = []\n",
    "    evidence_op:  List[Tuple[str, float]] = []\n",
    "    evidence_neg: List[Tuple[str, float]] = []\n",
    "    notes: List[str] = []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Ownership (About) first\n",
    "    # -------------------------------\n",
    "    about_exists = bool(about_url.strip())\n",
    "    if not about_exists:\n",
    "        # If you prefer a penalty instead of hard zero, swap with: z_own -= 1.0; p_own = sigmoid(z_own)\n",
    "        p_own = 0.0\n",
    "        notes.append(\"about_url missing; ownership prob forced to 0\")\n",
    "    else:\n",
    "        # If About explicitly contains a family-owned phrase, treat it as strong evidence\n",
    "        about_family = bool(FAMILY_ARTICLE_RX.search(about_text))\n",
    "        if about_family:\n",
    "            # stronger direct boosts for explicit About claims\n",
    "            ABOUT_FAMILY_OWN_BOOST = 1.2\n",
    "            ABOUT_FAMILY_OP_BOOST  = 0.9\n",
    "            z_own += ABOUT_FAMILY_OWN_BOOST\n",
    "            z_op  += ABOUT_FAMILY_OP_BOOST\n",
    "            evidence_own.append((\"about-text: explicit family-owned phrase\", round(ABOUT_FAMILY_OWN_BOOST,3)))\n",
    "            notes.append(\"about: explicit family-owned phrase detected; applied direct boost\")\n",
    "\n",
    "        # Apply negatives and positives -- but if about explicitly claims family-owned,\n",
    "        # avoid applying aggressive negatives from cookie/privacy boilerplate that may\n",
    "        # have been captured by earlier extraction.\n",
    "        if not about_family:\n",
    "            for rx in ANTI_FP:\n",
    "                if rx.search(about_text):\n",
    "                    z_own -= 0.6\n",
    "                    z_op  -= 0.6\n",
    "                    evidence_neg.append((\"anti-fp\", -0.6))\n",
    "            for rx, w in NEG:\n",
    "                for m in rx.finditer(about_text):\n",
    "                    z_own += w\n",
    "                    z_op  += w\n",
    "                    snippet = about_text[max(0, m.start()-60): m.end()+60]\n",
    "                    evidence_neg.append((snippet.strip(), w))\n",
    "\n",
    "        for rx, w in OWN_POS:\n",
    "            for m in rx.finditer(about_text):\n",
    "                z_own += w\n",
    "                snippet = about_text[max(0, m.start()-60): m.end()+60]\n",
    "                evidence_own.append((snippet.strip(), w))\n",
    "\n",
    "        p_own = sigmoid(z_own)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # NEWS — compute ONCE (safe defaults)\n",
    "    # -------------------------------------------------\n",
    "    z_news_own = 0.0\n",
    "    z_news_op  = 0.0\n",
    "    page_type  = \"none\"\n",
    "\n",
    "    if news_articles:\n",
    "        page_type = classify_news_page(news_articles)  # \"lineage\" or \"corporate\"\n",
    "        # prepare leader full-name set for stricter news matching\n",
    "        leader_full_set = set()\n",
    "        try:\n",
    "            leader_full_set = { (lambda n: (lambda toks: f\"{toks[0].lower()} {toks[-1].lower()}\" if len(toks)>=2 else \"\") ( [t for t in re.sub(r\"[^A-Za-z\\\\s]\",\" \", n).split() if t] ))(n) for n, _ in leaders_for_news if n }\n",
    "            leader_full_set = {n for n in leader_full_set if n}\n",
    "        except Exception:\n",
    "            leader_full_set = set()\n",
    "\n",
    "        z_news_own, z_news_op, ev_no, ev_np, ev_nn = score_news_articles(news_articles, page_type, leader_full_set)\n",
    "        # evidence\n",
    "        evidence_own.extend([(\"news: \" + s, w) for (s, w) in ev_no])\n",
    "        evidence_op.extend([(\"news: \" + s, w) for (s, w) in ev_np])\n",
    "        evidence_neg.extend([(\"news: \" + s, w) for (s, w) in ev_nn])\n",
    "\n",
    "        # Leader-name cross-match: if news mentions parsed leaders (surname overlap), give a bounded boost\n",
    "        try:\n",
    "            if leaders_for_news:\n",
    "                # Normalize to first+last lowercase form for stricter matching\n",
    "                def _norm_full(name: str) -> str:\n",
    "                    if not name:\n",
    "                        return \"\"\n",
    "                    toks = [t for t in re.sub(r\"[^A-Za-z\\\\s]\", \" \", name).split() if t]\n",
    "                    if len(toks) >= 2:\n",
    "                        return f\"{toks[0].lower()} {toks[-1].lower()}\"\n",
    "                    return \"\"\n",
    "\n",
    "                news_names = extract_all_person_names_from_news(news_articles)\n",
    "                news_full = {_norm_full(n) for n in news_names if n}\n",
    "                leader_full = {_norm_full(n) for n, _ in leaders_for_news if n}\n",
    "                common = {n for n in news_full & leader_full if n}\n",
    "                if common:\n",
    "                    # bounded boost: single full-name match -> 0.15, two or more -> 0.3\n",
    "                    boost = min(0.3, 0.15 * len(common))\n",
    "                    z_news_own += boost\n",
    "                    evidence_own.append((f\"news-leader name match: {', '.join(sorted(common))}\", round(boost,3)))\n",
    "                    notes.append(f\"news matched leader full-names; boost {round(boost,3)}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Ownership: fold in news contribution (always allowed)\n",
    "    # If this is an About page, check how many articles explicitly mention family-owned/operated\n",
    "    try:\n",
    "        # If we have an About page (about_url present), prefer About-derived family signals\n",
    "        if about_exists and news_articles:\n",
    "            family_hits = 0\n",
    "            for a in news_articles:\n",
    "                txt = (a.get('title','') or '') + ' ' + (a.get('body','') or '')\n",
    "                if FAMILY_ARTICLE_RX.search(txt):\n",
    "                    family_hits += 1\n",
    "\n",
    "            # also check About text directly for explicit family-owned phrasing\n",
    "            about_family = bool(FAMILY_ARTICLE_RX.search(about_text))\n",
    "            if about_family:\n",
    "                # count the about page as an additional corroborating hit\n",
    "                family_hits += 1\n",
    "                evidence_own.append((\"about-text: family phrase found\", 0.6))\n",
    "\n",
    "            if family_hits > 0:\n",
    "                mult = min(FAMILY_ARTICLE_MULT_CAP, 1.0 + FAMILY_ARTICLE_MULT * family_hits)\n",
    "                z_news_own *= mult\n",
    "                z_news_op  *= mult\n",
    "                notes.append(f\"about: boosted news by family-article multiplier x{mult:.2f} (hits={family_hits})\")\n",
    "            else:\n",
    "                # fallback modest boost for About pages\n",
    "                z_news_own *= ABOUT_NEWS_MULT\n",
    "                z_news_op  *= ABOUT_NEWS_MULT\n",
    "                notes.append(f\"about: applied fallback ABOUT_NEWS_MULT x{ABOUT_NEWS_MULT}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    z_own += z_news_own\n",
    "    p_own  = sigmoid(z_own)  # recompute in case news changed it\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Operation (HARD gate on leadership cluster)\n",
    "    # -------------------------------------------------\n",
    "    leaders_exists = bool(leaders_url.strip()) and any(k in leaders_url.lower() for k in LEADERSHIP_URL_KEYWORDS)\n",
    "    leaders_list = parse_leaders(leaders_summary) if leaders_exists else []\n",
    "\n",
    "    if not leaders_exists or not leaders_list:\n",
    "        # Soft gate: do NOT force operation to zero. Instead apply a conservative penalty\n",
    "        # when no leadership list or leaders page is missing, but allow news/about\n",
    "        # derived signals to partially rescue operation probability (capped).\n",
    "        penalty = -1.2  # conservative downward shift when leadership not observed\n",
    "        z_op = OPERATION_PRIOR_Z + penalty\n",
    "        # allow capped rescue from news-derived op signal\n",
    "        z_op += min(z_news_op, 0.8)\n",
    "        p_op = sigmoid(z_op)\n",
    "        notes.append(\"operation gate: no leadership cluster; soft gate applied (penalized), news rescue capped\")\n",
    "        # keep previous lineage rescue note if applicable\n",
    "        if page_type == \"lineage\" and z_news_op > 0:\n",
    "            notes.append(\"operation gate: lineage news rescue considered\")\n",
    "    else:\n",
    "        # Leadership-only surname clustering\n",
    "        ratio, ln, counts_w, total_w, shared_count_top, max_role_w_top, family_role_present = leadership_cluster(leaders_list)\n",
    "        evidence_op.append((\n",
    "            f\"surname cluster (leadership-only; diagnostic): top='{ln}' ratio={ratio:.2f} \",\n",
    "            f\"shared_count={shared_count_top} total_weight={total_w:.2f} counts={counts_w}\",\n",
    "            0.0,\n",
    "        ))\n",
    "\n",
    "        # Gate: require a leadership surname cluster (>=2 shared entries)\n",
    "        # Relaxation: accept single-entry surname if there are explicit family-role mentions\n",
    "        # or if About page explicitly claims family ownership. This helps sites that list\n",
    "        # one family-named leader alongside other non-family leaders but still indicate\n",
    "        # family operation (e.g. 'Family Member Since 2014').\n",
    "        family_cluster_hit = False\n",
    "        if ln and shared_count_top >= 2:\n",
    "            family_cluster_hit = True\n",
    "        else:\n",
    "            # allow rescue when 'family' is mentioned in leadership roles\n",
    "            if family_role_present and ratio >= 0.25:\n",
    "                family_cluster_hit = True\n",
    "            # allow rescue when About page explicitly states family-owned and ratio modest\n",
    "            if about_family and ln and ratio >= 0.20:\n",
    "                family_cluster_hit = True\n",
    "\n",
    "        if not family_cluster_hit:\n",
    "            # HARD gate — company not family operated (by your definition)\n",
    "            p_op = 0.0\n",
    "            z_op = OPERATION_PRIOR_Z\n",
    "            notes.append(\"operation gate: no leadership surname cluster; op prob forced to 0\")\n",
    "\n",
    "            # Optional: lineage-news rescue (capped)\n",
    "            if page_type == \"lineage\" and z_news_op > 0:\n",
    "                z_op += min(z_news_op, 0.8)  # cap rescue\n",
    "                p_op  = sigmoid(z_op)\n",
    "                notes.append(\"operation gate: lineage news rescue applied (capped)\")\n",
    "        else:\n",
    "            # Gate passed → allow news contribution\n",
    "            z_op += z_news_op\n",
    "\n",
    "            # Role contributions (token-based; with caps)\n",
    "            role_total = 0.0\n",
    "            board_total = 0.0\n",
    "            role_evidence: List[Tuple[str, float]] = []\n",
    "            for name, role in leaders_list:\n",
    "                w = role_weight(role)\n",
    "                if w > 0:\n",
    "                    role_evidence.append((f\"{name} — {role}\", w))\n",
    "                    role_total += w\n",
    "                    if 'board member' in norm_role(role):\n",
    "                        board_total += ROLE_PHRASES['board member']  # 0.4\n",
    "\n",
    "            # Apply caps\n",
    "            z_op += min(max(0.0, role_total - board_total), ROLE_CAP_TOTAL)\n",
    "            z_op += min(board_total, BOARD_CAP_TOTAL)\n",
    "            evidence_op.extend(role_evidence)\n",
    "\n",
    "            # Cluster boost based on ratio\n",
    "            if ratio >= 0.5:\n",
    "                z_op += 2.0\n",
    "            elif 0.35 <= ratio < 0.5:\n",
    "                z_op += 1.2\n",
    "            elif 0.25 <= ratio < 0.35:\n",
    "                z_op += 0.8\n",
    "\n",
    "            # Negatives on leadership summary (if any)\n",
    "            for rx, w in NEG:\n",
    "                for m in rx.finditer(leaders_summary):\n",
    "                    z_own += w\n",
    "                    z_op  += w\n",
    "                    snippet = leaders_summary[max(0, m.start()-60): m.end()+60]\n",
    "                    evidence_neg.append((snippet.strip(), w))\n",
    "\n",
    "            p_op = sigmoid(z_op)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Final AND via Harmonic Mean (soft-zero)\n",
    "    # -------------------------------------------------\n",
    "    EPS = 1e-6  # numerical floor\n",
    "    p_and = 2.0 / (1.0 / max(p_own, EPS) + 1.0 / max(p_op, EPS))\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"family_owned_and_operated_prob\": round(p_and, 3),\n",
    "        \"ownership\": {\n",
    "            \"prob\": round(p_own, 3),\n",
    "            \"logit\": round(z_own, 3),\n",
    "            \"evidence\": evidence_own,\n",
    "        },\n",
    "        \"operation\": {\n",
    "            \"prob\": round(p_op, 3),\n",
    "            \"logit\": round(z_op, 3),\n",
    "            \"evidence\": evidence_op,\n",
    "        },\n",
    "        \"negatives\": evidence_neg,\n",
    "        \"notes\": notes,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74115778-c08e-41b0-b221-fb89dc73b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: https://cgschmidt.com\n",
      "Done: https://cgschmidt.com in 116.73s\n",
      "{'url': 'https://cgschmidt.com', 'family_owned_and_operated_prob': 0.946, 'ownership': {'prob': 0.9, 'logit': 2.2, 'evidence': [('t better than a solid partnership and honest conversations. Five generations of quality, caring, and commitment At CG Schmidt, we have b', 2.2), ('t better than a solid partnership and honest conversations. Five generations of quality, caring, and commitment At CG Schmidt, we have b', 1.6), ('news-leader name match: eric schmidt, mark lillesand, mike abuls, rick schmidt, sarah dunn', 0.3)]}, 'operation': {'prob': 0.997, 'logit': 5.93, 'evidence': [('news: 5 | News & Events Eric Schmidt, Ryan Schmidt & Josh Schmitz Named 2025 Newsmakers of the Year by The Daily Reporter CG Schmid', 0.272), ('news: team—Eric Schmidt, Ryan Schmidt and Josh Schmitz—have been named 2025 Newsmakers of the Year by The Daily Reporter. These di', 0.272), ('news: ng a strong, comp Eric Schmidt, Ryan Schmidt & Josh Schmitz Named 2025 Newsmakers of the Year by The Daily Reporter', 0.272), ('news: kers of the Year by The Daily Reporter. These distinguished honors recognize individuals who have made significant contributio', 0.136), ('news: k Lillesand – Senior Vice President Mark Lillesand has been promoted to Senior Vice President. Throughout his twenty-four-year c', 0.136), ('news: Business Development & Client Services Sarah Dunn has been promoted to Senior Vice President of Business Development and Client', 0.136), ('news: n Smith – Director of Field Operations Aaron Smith has been named Director of Field Operations.\\xa0 Aaron has served as a superi', 0.136), ('news: ith has been named Director of Field Operations.\\xa0 Aaron has served as a superintendent, managing field teams on many of CG Schmid', 0.068), ('news: n Smith – Director of Field Operations Aaron Smith has been named Director of Field Operations.\\xa0 Aaron has served as a superintendent,', 0.113), (\"surname cluster (leadership-only; diagnostic): top='schmidt' ratio=0.34 \", \"shared_count=2 total_weight=7.70 counts={'lillesand': 1.0, 'chovanec': 1.0, 'unger': 1.0, 'schmidt': 2.6, 'abuls': 1.2, 'rakowski': 0.9}\", 0.0), ('Mark Lillesand — Executive Vice President', 1.0), ('Dan Chovanec — Senior Vice President', 1.0), ('Bryce Unger — Vice President', 1.0), ('Rick Schmidt — Chairman of the Board', 0.8), ('Eric Schmidt — President & CEO', 1.8), ('Mike Abuls — Chief Operating Officer, Executive Vice President', 1.2), ('Todd Rakowski — Chief Financial Officer & Vice President of Finance', 0.9)]}, 'negatives': [], 'notes': ['news matched leader full-names; boost 0.3', 'about: boosted news by family-article multiplier x2.00 (hits=4)']}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Run \n",
    "# ---------------------------\n",
    "sites = [\n",
    "\"https://www.pyramaxbank.com\",\n",
    "\"https://www.1901inc.com/\",\n",
    "\"www.appliedtechproducts.com\",\n",
    "\"www.aeincorporated.com\",\n",
    "\"www.abrjobs.com\",\n",
    "\"https://www.absoluteconcrete.com\",\n",
    "\"https://www.acbusinessmedia.com\",\n",
    "\"http://www.accelgen.com\",\n",
    "\"http://www.accuratefab.net\",\n",
    "\"http://www.acieta.com\"\n",
    "] # input list of werbsite names in the following format: [\"site1.com\", \"site2.com\", ...]\n",
    "\n",
    "URLS = [u if u.startswith((\"http://\", \"https://\")) else \"https://\" + u for u in ensure_list(sites)]\n",
    "\n",
    "results, leader_rows = run_all(URLS)\n",
    "\n",
    "\n",
    "recs = []\n",
    "for i in range(len(results)):\n",
    "    recs.append(score_record(results[i]))\n",
    "\n",
    "for r in recs:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: c:\\Users\\VB948\\OneDrive - R&R Insurance\\Desktop\\RR Projects\\family_owned_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save predictor results to Excel\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefer `recs` (scored records) then `results` (raw) then `records` (test input)\n",
    "ws = globals()\n",
    "data = None\n",
    "for name in (\"recs\", \"results\", \"records\", \"out_records\", \"results_list\"):\n",
    "    if name in ws:\n",
    "        data = ws[name]\n",
    "        break\n",
    "\n",
    "if data is None:\n",
    "    print(\"No results variable found in notebook namespace. Nothing saved.\")\n",
    "else:\n",
    "    # Normalize list of dicts to flat table\n",
    "    rows = []\n",
    "    for r in data:\n",
    "        row = {}\n",
    "        row[\"url\"] = r.get(\"url\")\n",
    "        row[\"family_owned_and_operated_prob\"] = r.get(\"family_owned_and_operated_prob\")\n",
    "\n",
    "        # Ownership\n",
    "        own = r.get(\"ownership\", {}) or {}\n",
    "        row[\"ownership_prob\"] = own.get(\"prob\")\n",
    "        row[\"ownership_logit\"] = own.get(\"logit\")\n",
    "        ev_own = own.get(\"evidence\") or []\n",
    "        # evidence is list of tuples/snippets; serialize compactly\n",
    "        def serialize_evidence(ev_list):\n",
    "            out = []\n",
    "            for it in ev_list:\n",
    "                try:\n",
    "                    if isinstance(it, (list, tuple)) and len(it) >= 2:\n",
    "                        snippet = str(it[0]).replace(\"\\n\", \" \")\n",
    "                        weight = it[1]\n",
    "                        out.append(f\"{snippet} || {weight}\")\n",
    "                    else:\n",
    "                        out.append(str(it))\n",
    "                except Exception:\n",
    "                    out.append(str(it))\n",
    "            return \" ;; \".join(out)\n",
    "\n",
    "        row[\"ownership_evidence\"] = serialize_evidence(ev_own)\n",
    "\n",
    "        # Operation\n",
    "        op = r.get(\"operation\", {}) or {}\n",
    "        row[\"operation_prob\"] = op.get(\"prob\")\n",
    "        row[\"operation_logit\"] = op.get(\"logit\")\n",
    "        row[\"operation_evidence\"] = serialize_evidence(op.get(\"evidence\") or [])\n",
    "\n",
    "        # Negatives and notes\n",
    "        negs = r.get(\"negatives\") or []\n",
    "        row[\"negatives\"] = \" ;; \".join([str(x[0]) if isinstance(x, (list, tuple)) else str(x) for x in negs])\n",
    "        row[\"notes\"] = \" ;; \".join(r.get(\"notes\") or [])\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_path = Path.cwd() / \"family_owned_results.xlsx\"\n",
    "    try:\n",
    "        df.to_excel(out_path, index=False)\n",
    "        print(f\"Saved results to: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save Excel file:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
